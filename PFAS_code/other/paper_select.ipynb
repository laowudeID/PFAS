{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literature screening model\n",
    "\n",
    "文献筛选模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10375, 67370)\n",
      "(10375, 1000)\n",
      "SVM CV scores: [0.97495183 0.96917148 0.97302505 0.96435453 0.97591522 0.97492768\n",
      " 0.97203472 0.96817743 0.975892   0.97299904]\n",
      "TF-IDF_PCA_SVM predicted 1 count: 437\n",
      "(10375, 1000)\n",
      "RF CV scores: [0.95375723 0.95375723 0.9566474  0.95472062 0.95375723 0.95660559\n",
      " 0.95564127 0.95371263 0.95564127 0.95371263]\n",
      "TF-IDF_PCA_RF predicted 1 count: 495\n",
      "(10375, 1000)\n",
      "NN CV scores: [0.96242775 0.96050096 0.95279383 0.96628131 0.96146435 0.97492768\n",
      " 0.97878496 0.96721311 0.9710704  0.96624879]\n",
      "TF-IDF_PCA_NN predicted 1 count: 496\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "file_path = r'D:/wyy/data record/paper search/train_dataset.csv'\n",
    "df = pd.read_csv(file_path,encoding=\"unicode_escape\")\n",
    "df[\"txt\"] = df[\"title\"] + df[\"abstract\"]\n",
    "df[\"txt\"] = df[\"txt\"].str.lower()\n",
    "df[\"txt\"] = df[\"txt\"].astype(str)\n",
    "df[\"txt\"] = df[\"txt\"].apply(lambda x: re.sub(r'\\d+|[^\\w\\s]', '', x))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df[\"txt\"] = df[\"txt\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "def feature_extraction(txt, method=\"TF-IDF\"):\n",
    "    if method == \"TF-IDF\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif method == \"N-gram\":\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    elif method == \"Word2Vec\":\n",
    "        model = Word2Vec(txt, window=5, min_count=1, workers=4)\n",
    "        vectors = np.array([np.mean([model.wv[word] for word in sentence.split() if word in model.wv], axis=0) for sentence in txt])\n",
    "        return vectors\n",
    "\n",
    "    X = vectorizer.fit_transform(txt)\n",
    "    print(X.shape)\n",
    "    return X.toarray()\n",
    "\n",
    "def dimension_reduction(X, method=\"PCA\"):\n",
    "    n_components = 1000\n",
    "    if method == \"PCA\":\n",
    "        dr = PCA(n_components=n_components)\n",
    "    elif method == \"LDA\":\n",
    "        dr = LatentDirichletAllocation(n_components=n_components)\n",
    "    elif method == \"t-SNE\":\n",
    "        dr = TSNE(n_components=3)\n",
    "    \n",
    "    X_reduced = dr.fit_transform(X)\n",
    "    return X_reduced\n",
    "\n",
    "def model_fit(X, y, model_name=\"SVM\"):\n",
    "    print(X.shape)\n",
    "    if model_name == \"SVM\":\n",
    "        clf = SVC()\n",
    "    elif model_name == \"RF\":\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=112341)\n",
    "    elif model_name == \"NN\":\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=500, random_state=42)\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=10)\n",
    "    print(f\"{model_name} CV scores:\", cv_scores)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def save_results(clf, X, model_name, output_dir):\n",
    "    preds = clf.predict(X)\n",
    "    count_ones = np.sum(preds)\n",
    "    print(f\"{model_name} predicted 1 count:\", count_ones)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    file_name = f\"{model_name}-{timestamp}.csv\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "    pd.DataFrame({'mark': df.mark, 'title': df.title, 'Chinese': df.Chinese, 'outcome': preds}).to_csv(output_path, index=False,encoding=\"utf-8-sig\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fem_name = \"TF-IDF\"\n",
    "    X = feature_extraction(df[\"txt\"], method=fem_name)\n",
    "    drm_name = \"PCA\"\n",
    "    X_reduced = dimension_reduction(X, method=drm_name)\n",
    "    y = df[\"mark\"]\n",
    "\n",
    "    model_names = [\"SVM\", \"RF\", \"NN\"]\n",
    "    for model_name in model_names:\n",
    "        clf = model_fit(X_reduced, y, model_name=model_name)\n",
    "        full_name = fem_name + \"_\" + drm_name + \"_\" + model_name\n",
    "        save_results(clf, X_reduced, full_name, output_dir=\"D:\\\\wyy\\\\data record\\\\paper search\\\\output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv(\"D:/wyy/data record/paper search/train_dataset.csv\",encoding=\"unicode_escape\")\n",
    "df_t = df_t[(df_t[\"title\"].notna())&(df_t[\"abstract\"].notna())]\n",
    "df_f = pd.read_csv(\"D:/wyy/data record/paper search/forecast_dataset.csv\",encoding=\"unicode_escape\")\n",
    "df_f = df_f[(df_f[\"title\"].notna())&(df_f[\"abstract\"].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "df_t = pd.read_csv(\"D:/wyy/data record/paper search/train_dataset.csv\",encoding=\"unicode_escape\")\n",
    "df_t = df_t[(df_t[\"title\"].notna())&(df_t[\"abstract\"].notna())]\n",
    "\n",
    "df_f = pd.read_csv(\"D:/wyy/data record/paper search/forecast_dataset.csv\",encoding=\"unicode_escape\")\n",
    "df_f = df_f[(df_f[\"title\"].notna())&(df_f[\"abstract\"].notna())]\n",
    "\n",
    "\n",
    "def df_merge(df):\n",
    "    df[\"txt\"] = df[\"title\"] + df[\"abstract\"]\n",
    "    df[\"txt\"] = df[\"txt\"].str.lower()\n",
    "    df[\"txt\"] = df[\"txt\"].astype(str)\n",
    "    df[\"txt\"] = df[\"txt\"].apply(lambda x: re.sub(r'\\d+|[^\\w\\s]', '', x))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df[\"txt\"] = df[\"txt\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    return df\n",
    "\n",
    "df_f_f = df_f[~df_f['title'].isin(df_t['title'])]\n",
    "df_concat = pd.concat([df_t, df_f_f])\n",
    "print(df_concat.shape)\n",
    "df_concat = df_merge(df_concat)\n",
    "\n",
    "\n",
    "def save_results(clf, X, model_name, output_dir):\n",
    "    preds = clf.predict(X)\n",
    "    count_ones = np.sum(preds)\n",
    "    print(f\"{model_name} predicted 1 count:\", count_ones)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    file_name = f\"{model_name}-{timestamp}.csv\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "    pd.DataFrame({'Mark': df.Mark, 'Article Title': df.title, 'outcome': preds}).to_csv(output_path, index=False,encoding=\"gbk\")\n",
    "if __name__ == \"__main__\":\n",
    "    fem_name = \"TF-IDF\"\n",
    "    X = feature_extraction(df_concat[\"txt\"], method=fem_name)\n",
    "    drm_name = \"PCA\"\n",
    "    X_reduced = dimension_reduction(X, method=drm_name)\n",
    "\n",
    "    X_reduced_test = X_reduced[:10403,:]\n",
    "    X_reduced_train = X_reduced[10403:,:]\n",
    "\n",
    "    df_concat_loc = df_concat.iloc[:10403,:]\n",
    "    y = df_concat_loc[\"mark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10403, 3)\n",
      "RF CV scores: [0.95869356 0.95869356 0.94908742 0.95961538 0.96538462 0.97211538\n",
      " 0.96538462 0.96634615 0.96634615 0.95576923]\n",
      "RF predicted need paper count: 118.0\n",
      "Index(['ï»¿authors', 'title', 'Chinese', 'source', 'DOI', 'abstract', 'year',\n",
      "       'mark'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "model_name = \"RF\"\n",
    "clf = model_fit(X_reduced_test, y, model_name=model_name)\n",
    "preds = clf.predict(X_reduced_train)\n",
    "\n",
    "count_ones = np.sum(preds)\n",
    "print(f\"{model_name} predicted need paper count:\", count_ones)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "file_name = f\"{model_name}-{timestamp}.csv\"\n",
    "output_dir=\"D:\\\\wyy\\\\data record\\\\paper search\\\\output\"\n",
    "output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "print(df_f.columns)\n",
    "df_concat = df_concat.rename(columns={'ï»¿authors':'authors'})\n",
    "df_n_train = df_concat.iloc[10403:,:]\n",
    "pd.DataFrame({'authors': df_n_train.authors, 'title': df_n_train.title, \n",
    "                'source': df_n_train.source,   'DOI': df_n_train.DOI, \n",
    "                'year': df_n_train.year,       'abstract': df_n_train.abstract,\n",
    "                'outcome': preds,              'Chinese': df_n_train.Chinese,\n",
    "                }).to_csv(output_path, index=False,encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyyrnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
