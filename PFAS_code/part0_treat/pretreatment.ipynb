{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing of data including: data screening, identification of outliers, weighted average of data based on sample size, gridding  \n",
    "\n",
    "对数据进行预处理，包括：数据筛选，识别异常值，根据样本数对数据进行加权平均，网格化  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import myfunction as mf\n",
    "path_data_raw = \"C:/Users/dell/OneDrive/file/\"\n",
    "path_country_nc = \"C:/Users/dell/OneDrive/file/nc/\"\n",
    "path_one_spdb = 'C:/Users/dell/OneDrive/file/SPDB/'\n",
    "\n",
    "drive_letter = 'E:'\n",
    "path_opp = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/pretreatment/\"\n",
    "path_pre = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/pretreatment/\"\n",
    "path_match = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/match/\"\n",
    "path_picture = drive_letter + \"/wyy/code_project/running_outcome/final_fig/SPDB/part0_treat/pretreatment/\"\n",
    "path_temp = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/temp/\"\n",
    "\n",
    "path_ng_fig = drive_letter + \"/wyy/code_project/running_outcome/final_fig/ng_fig/\"\n",
    "list_color = [\"#ee877c\", \"#8bd0e3\", \"#6abeae\", \"#808eaf\", \"#f7bba8\", \"#acb4cc\", \"#b5e0d5\", \"#e86462\", \"#a89687\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of functions to identify outliers\n",
    "# 识别异常值的函数准备\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyod.models.iforest import IForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_outlier(df_x, col_1, col_2, col_3, int_fraction):\n",
    "    \"\"\"\n",
    "    col_1, col_2, col_3: Three variables for identifying outliers\n",
    "    int_fraction: Percentage of outliers\n",
    "    ---\n",
    "    col_1, col_2, col_3: 帮助识别异常值的三个变量\n",
    "    int_fraction: 异常值百分比\n",
    "    \"\"\"\n",
    "    df = df_x.copy()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df[[col_1, col_2, col_3]] = scaler.fit_transform(df[[col_1, col_2, col_3]])\n",
    "    df[[col_1, col_2, col_3]].head()\n",
    "    X1 = df[col_1].values.reshape(-1, 1)\n",
    "    X2 = df[col_2].values.reshape(-1, 1)\n",
    "    X3 = df[col_3].values.reshape(-1, 1)\n",
    "    X = np.concatenate((X1, X2, X3), axis=1)\n",
    "    random_state = np.random.RandomState(1024)\n",
    "    outliers_fraction = int_fraction\n",
    "    classifiers = {\n",
    "        'Isolation Forest': IForest(contamination=outliers_fraction, random_state=random_state),\n",
    "    }\n",
    "    xx, yy, zz = np.meshgrid(np.linspace(0, 1, 200), np.linspace(0, 1, 200), np.linspace(0, 1, 200))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # 用该模型训练X\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1\n",
    "        # 计算样本点到分割超平面的函数距离，即各个数据的异常得分\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        dfx = df\n",
    "        dfx['outlier'] = y_pred.tolist()\n",
    "        IX1 = np.array(dfx[col_1][dfx['outlier'] == 0]).reshape(-1, 1)\n",
    "        IX2 = np.array(dfx[col_2][dfx['outlier'] == 0]).reshape(-1, 1)\n",
    "        IX3 = np.array(dfx[col_3][dfx['outlier'] == 0]).reshape(-1, 1)\n",
    "        OX1 = dfx[col_1][dfx['outlier'] == 1].values.reshape(-1, 1)\n",
    "        OX2 = dfx[col_2][dfx['outlier'] == 1].values.reshape(-1, 1)\n",
    "        OX3 = dfx[col_3][dfx['outlier'] == 1].values.reshape(-1, 1)\n",
    "        print('OUTLIERS : ', n_outliers, 'INLIERS : ', n_inliers, clf_name)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(IX1, IX2, IX3, c='white', s=6, edgecolor='k')\n",
    "        ax.scatter(OX1, OX2, OX3, c='red', s=6, edgecolor='k')\n",
    "        plt.xlim((0, 1))\n",
    "        plt.ylim((0, 1))\n",
    "        plt.title(clf_name)\n",
    "        plt.show()\n",
    "        return dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data screening\n",
    "# 筛选数据\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from scipy.spatial import cKDTree\n",
    "path_data_raw = \"C:/Users/dell/OneDrive/file/\"\n",
    "df_data_raw = pd.read_csv(path_data_raw + \"lr_use.csv\")\n",
    "nc_file_path = \"C:/Users/dell/OneDrive/file/nc/country_id_0.1x0.1.nc\"\n",
    "nc_data = Dataset(nc_file_path, 'r')\n",
    "lon = nc_data.variables['lon'][:]\n",
    "lat = nc_data.variables['lat'][:]\n",
    "country_id = nc_data.variables['country_id'][:]\n",
    "lon_lat_grid = np.array(np.meshgrid(lon, lat))\n",
    "grid_shape = lon_lat_grid.shape[1:]\n",
    "lon_lat_grid = lon_lat_grid.reshape(2, -1).T\n",
    "tree = cKDTree(lon_lat_grid)\n",
    "points_to_query = df_data_raw[['lon', 'lat']].values\n",
    "distances, indices = tree.query(points_to_query)\n",
    "df_data_raw['country_id'] = country_id.reshape(-1)[indices]\n",
    "print(df_data_raw.head())\n",
    "print(df_data_raw.isnull().sum())\n",
    "dict_inf = {\"sp\":['phylum',\"class\",\"length_last\",\"weight_last\",\"troph_last\", 'rank','family','genus','species'],\n",
    "             \"po\":[\"po_chain\",\"posname\",'posid','po_carbon','po_f_carbon','po_classification'],\n",
    "             'country':['country_code', 'country','continent','income'],\n",
    "             'habit':['habitat']}\n",
    "df_data_raw2 = mf.append_inf(df_data_raw, dict_inf)\n",
    "df_data_raw_select = df_data_raw2[(df_data_raw2['posid'].notna())&(df_data_raw2['rank']=='SPECIES')]\n",
    "print(df_data_raw2.shape)\n",
    "print(df_data_raw_select.shape)\n",
    "print(df_data_raw_select.isnull().sum())\n",
    "df_data_raw_select = mf.df_merge_lat_lon(df_data_raw_select)\n",
    "print(df_data_raw_select.dtypes)\n",
    "df_data_raw_select.to_csv(path_pre + \"lr_use_pre_select.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the data situation\n",
    "# 查看数据情况\n",
    "df_data_raw_select = pd.read_csv(path_pre + \"lr_use_pre_select.csv\")\n",
    "print(df_data_raw_select.shape)\n",
    "list_items = ['posname','class','po_chain','habitat','income','continent','organ']\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "for item in list_items:\n",
    "    # print('-------------',item,'--------------------')\n",
    "    df_item = mf.col_value(df_data_raw_select,item)\n",
    "    print(df_item)\n",
    "    # print('-------------',item,'--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further screening of data\n",
    "# 进一步筛选数据\n",
    "df_data_pre = pd.read_csv(path_pre + \"lr_use_pre_select.csv\")\n",
    "print(df_data_pre.shape)\n",
    "list_posname = mf.col_value(df_data_pre,'posname')['posname'].unique().tolist()[0:15]\n",
    "list_class = mf.col_value(df_data_pre,'class')['class'].unique().tolist()[0:3]\n",
    "list_organ = mf.col_value(df_data_pre,'organ')['organ'].unique().tolist()[0:4]\n",
    "df_data_pre_select = df_data_pre[(df_data_pre['posname'].isin(list_posname))&(df_data_pre['class'].isin(list_class))&(df_data_pre['organ'].isin(list_organ))]\n",
    "df_data_pre_select['time_year'] = df_data_pre_select['time_year'].astype(float)\n",
    "df_data_pre_select['time_year'] = df_data_pre_select['time_year'].astype(int)\n",
    "df_data_pre_select = df_data_pre_select[(df_data_pre_select['time_year']<=2020)&(df_data_pre_select['time_year']>=2000)]\n",
    "print(df_data_pre_select.shape)\n",
    "df_data_pre_select.to_csv(path_pre + \"lr_select.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preset one per cent outliers\n",
    "# 预设百分之一的异常值\n",
    "df_data_pre_select = pd.read_csv(path_pre + \"lr_select.csv\")\n",
    "print(df_data_pre_select.shape)\n",
    "df_icicle_w_o_outlier = df_data_pre_select.copy()\n",
    "\n",
    "le_ind = LabelEncoder()\n",
    "po_le = le_ind.fit_transform(df_icicle_w_o_outlier[\"posname\"])\n",
    "df_icicle_w_o_outlier[\"po_copy\"] = po_le\n",
    "df_icicle_w_o_outlier['value_copy'] = (df_icicle_w_o_outlier['value'] - df_icicle_w_o_outlier.groupby('po_copy')['value'].transform('min')) / (df_icicle_w_o_outlier.groupby('po_copy')['value'].transform('max') - df_icicle_w_o_outlier.groupby('po_copy')['value'].transform('min'))\n",
    "df_icicle_w_o_outlier[\"time_year_copy\"] = df_icicle_w_o_outlier[\"time_year\"]\n",
    "df_icicle_w_o_outlier = get_outlier(df_icicle_w_o_outlier,\"time_year_copy\", \"po_copy\",\"value_copy\", 0.01)\n",
    "\n",
    "df_icicle_w_o_outlier = df_icicle_w_o_outlier[df_icicle_w_o_outlier[\"outlier\"]==0]\n",
    "\n",
    "list_3 = [\"habitat\",\"spid\",\"posname\",\"lon\",\"lat\",\"organ\",\"time_year\", \"value\", 'n']\n",
    "list_4 = [\"habitat\",\"spid\",\"lon\",\"lat\",\"organ\",\"time_year\",'n']\n",
    "df_icicle_w_o_outlier = df_icicle_w_o_outlier[list_3]\n",
    "print(df_icicle_w_o_outlier.shape)\n",
    "df_icicle_w_o_outlier.to_csv(path_pre + \"lr_outlier.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted average of data based on sample size\n",
    "# 根据样本数对数据进行加权平均\n",
    "df_outlier = pd.read_csv(path_pre + \"lr_outlier.csv\")\n",
    "print(df_outlier.shape)\n",
    "list_1 = [\"habitat\",\"spid\",\"posname\",\"lon\",\"lat\",\"organ\",\"time_year\"]\n",
    "list_2 = [\"habitat\",\"spid\",\"posname\",\"lon\",\"lat\",\"organ\",\"time_year\", \"n\",\"value\"]\n",
    "list_3 = [\"habitat\",\"spid\",\"posname\",\"lon\",\"lat\",\"organ\",\"time_year\", \"value\"]\n",
    "df_icicle = df_outlier[list_2]\n",
    "df_icicle_w = mf.get_weight_count(df_icicle,list_1,'mean') \n",
    "df_icicle_w_o = df_icicle_w[list_3]\n",
    "print(df_icicle_w_o.shape)\n",
    "df_icicle_w_o.to_csv(path_pre + \"lr_weight_avg.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Gridding\n",
    "# 数据网格化\n",
    "df_weight = pd.read_csv(path_pre + \"lr_weight_avg.csv\")\n",
    "print(df_weight.shape)\n",
    "int_grid = 1\n",
    "def lat_lon_to_grid(lat, lon, grid_size=1):\n",
    "    return round(lat / grid_size) * grid_size, round(lon / grid_size) * grid_size\n",
    "\n",
    "\n",
    "df_weight['lat_grid'], df_weight['lon_grid'] = zip(*df_weight.apply(lambda row: lat_lon_to_grid(row['lat'], row['lon'], int_grid), axis=1))\n",
    "\n",
    "df_grid = df_weight.groupby(['lat_grid', 'lon_grid', 'habitat', 'spid', 'organ', 'time_year', 'posname'])['value'].mean().reset_index()\n",
    "print(df_grid.shape)\n",
    "df_grid.to_csv(path_pre + \"lr_grid_avg.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dummy code with its corresponding categorical variable\n",
    "# 保存虚拟编码与之对应的分类变量\n",
    "\n",
    "list_need_sort = ['class','order','po_classification','posname','organ','habitat',\"family\",\"genus\",\"spid\"]\n",
    "dict_inf = {\"sp\":[\"class\",'order','family','genus','species'],\n",
    "             \"pon\":['po_carbon','po_f_carbon','po_classification']}\n",
    "df_grid = pd.read_csv(path_pre + 'lr_grid.csv')\n",
    "df_sort = mf.append_inf(df_grid,dict_inf)\n",
    "for var in list_need_sort:\n",
    "    df_var = mf.col_describe(df_sort,var)\n",
    "    df_var = df_var[[var,\"50%\"]]\n",
    "    df_var = df_var.sort_values(by=\"50%\", ascending=True)\n",
    "    df_var = df_var.reset_index(drop=True)\n",
    "    df_var['id'] = df_var.index\n",
    "    df_var.to_csv(path_one_spdb + var + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data and identifying outliers\n",
    "# 筛选数据及识别异常值\n",
    "str_sw_remark = 's7'\n",
    "df_sw_all = pd.read_csv(path_data_raw + \"sw.csv\")\n",
    "list_select_source = ['paper','norman', 'gmp', 'egle']\n",
    "df_sw_select = df_sw_all[df_sw_all['source'].isin(list_select_source)]\n",
    "print(df_sw_select.shape)\n",
    "df_sw_select.to_csv(path_temp + 'sw'+str_sw_remark+'_select.csv', index=False)\n",
    "\n",
    "df_data_pre_select = pd.read_csv(path_temp + 'sw'+str_sw_remark+'_select.csv')\n",
    "\n",
    "df_data_pre_select = df_data_pre_select.rename(columns={'sw_value': 'value'})\n",
    "df_data_pre_select = df_data_pre_select[df_data_pre_select['value']>0]\n",
    "print(df_data_pre_select.shape)\n",
    "df_icicle_w_o_outlier = df_data_pre_select.copy()\n",
    "print(df_icicle_w_o_outlier.dtypes)\n",
    "\n",
    "df_icicle_w_o_outlier[\"po_copy\"] = df_icicle_w_o_outlier[\"posname\"]\n",
    "df_icicle_w_o_outlier['value_copy'] = (df_icicle_w_o_outlier['value'] - df_icicle_w_o_outlier.groupby('po_copy')['value'].transform('min')) / (df_icicle_w_o_outlier.groupby('po_copy')['value'].transform('max') - df_icicle_w_o_outlier.groupby('po_copy')['value'].transform('min'))\n",
    "df_icicle_w_o_outlier[\"year_copy\"] = df_icicle_w_o_outlier[\"year\"]\n",
    "df_icicle_w_o_outlier = get_outlier(df_icicle_w_o_outlier,\"year_copy\", \"po_copy\",\"value_copy\", 0.01)\n",
    "\n",
    "df_icicle_w_o_outlier = df_icicle_w_o_outlier[df_icicle_w_o_outlier[\"outlier\"]==0]\n",
    "\n",
    "list_3 = [\"lon\",\"lat\",\"posname\",\"year\", \"value\", 'n']\n",
    "\n",
    "df_icicle_w_o_outlier = df_icicle_w_o_outlier[list_3]\n",
    "print(df_icicle_w_o_outlier.shape)\n",
    "df_icicle_w_o_outlier.to_csv(path_temp + 'sw'+str_sw_remark+'_outlier_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted average of data based on sample size\n",
    "# 根据样本数对数据进行加权平均\n",
    "df_outlier = pd.read_csv(path_temp + 'sw'+str_sw_remark+'_outlier_1.csv')\n",
    "print(df_outlier.shape)\n",
    "list_1 = [\"posname\",\"lon\",\"lat\",\"year\"]\n",
    "list_2 = [\"posname\",\"lon\",\"lat\",\"year\", \"n\",\"value\"]\n",
    "list_3 = [\"posname\",\"lon\",\"lat\",\"year\", \"value\"]\n",
    "df_icicle = df_outlier[list_2]\n",
    "\n",
    "df_icicle_w = mf.get_weight_count(df_icicle,list_1,'mean') \n",
    "\n",
    "df_icicle_w_o = df_icicle_w[list_3]\n",
    "print(df_icicle_w_o.shape)\n",
    "df_icicle_w_o.to_csv(path_temp + 'sw'+str_sw_remark+'_weight_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Gridding\n",
    "# 数据网格化\n",
    "df_weight = pd.read_csv(path_temp + 'sw'+str_sw_remark+'_weight_1.csv')\n",
    "print(df_weight.shape)\n",
    "int_grid = 1\n",
    "def lat_lon_to_grid(lat, lon, grid_size=1):\n",
    "    return round(lat / grid_size) * grid_size, round(lon / grid_size) * grid_size\n",
    "\n",
    "\n",
    "df_weight['lat_grid'], df_weight['lon_grid'] = zip(*df_weight.apply(lambda row: lat_lon_to_grid(row['lat'], row['lon'], int_grid), axis=1))\n",
    "\n",
    "df_grid = df_weight.groupby(['lat_grid', 'lon_grid', 'year', 'posname'])['value'].mean().reset_index()\n",
    "\n",
    "print(df_grid.shape)\n",
    "df_grid = df_grid.rename(columns={'posname':'id'})\n",
    "posname_df = pd.read_csv(path_one_spdb + 'posname.csv')\n",
    "posname_df = posname_df.drop(columns=[\"50%\"])\n",
    "df_grid = df_grid.merge(posname_df, how='left', on='id').drop('id', axis=1)\n",
    "\n",
    "df_grid.to_csv(path_pre + 'sw'+str_sw_remark+'_grid_1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
