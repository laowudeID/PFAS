{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of geospatial data and matching to waterbody and fish data\n",
    "准备地理空间数据，并匹配到水体和鱼类数据中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import myfunction as mf\n",
    "path_data_raw = \"C:/Users/dell/OneDrive/file/\"\n",
    "path_country_nc = \"C:/Users/dell/OneDrive/file/nc\"\n",
    "path_one_spdb = 'C:/Users/dell/OneDrive/file/SPDB/'\n",
    "drive_letter = 'E:'\n",
    "\n",
    "path_pre = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/pretreatment/\"\n",
    "path_match = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/match/\"\n",
    "path_var_data = drive_letter + \"/wyy/SPDB_database/data/raw/\"\n",
    "path_match_geo = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/match/geo/\"\n",
    "\n",
    "meta_name = \"meta_data.csv\"\n",
    "mark_num = \"25\"\n",
    "list_color = [\"#ee877c\", \"#8bd0e3\", \"#6abeae\", \"#808eaf\", \"#f7bba8\", \"#acb4cc\", \"#b5e0d5\", \"#e86462\", \"#a89687\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEO-DATA-PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the required data tables\n",
    "# 生成需要的数据表格\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "lat_range = np.arange(-90, 90, 1)\n",
    "lon_range = np.arange(-180, 180, 1)\n",
    "year_range = np.arange(2000, 2021, 1)\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'lat_grid': np.repeat(lat_range, len(lon_range) * len(year_range)),\n",
    "    'lon_grid': np.tile(np.repeat(lon_range, len(year_range)), len(lat_range)),\n",
    "    'year': np.tile(year_range, len(lat_range) * len(lon_range))\n",
    "})\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'lat_grid': np.repeat(lat_range, len(lon_range)),\n",
    "    'lon_grid': np.tile(lon_range, len(lat_range))\n",
    "})\n",
    "\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "\n",
    "df1.to_csv(path_match + \"lat_lon_year.csv\", index=False)\n",
    "df2.to_csv(path_match + \"lat_lon.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of geospatial variables\n",
    "# 预处理地理空间变量\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————————\n",
    "int_grid = 1\n",
    "data_type = \"raw\"\n",
    "# ——————————————————————————————————————————————————————————————————\n",
    "\n",
    "nc_dir_year = path_var_data + str(int_grid) + \"\\\\year\"\n",
    "nc_dir_one = path_var_data + str(int_grid) + \"\\\\one\"\n",
    "\n",
    "\n",
    "df_meta = pd.read_csv(path_data_raw + meta_name, encoding=\"utf-8\")\n",
    "list_var = df_meta['var_name'][(df_meta['file_type']=='nc')&(df_meta['var_select' + mark_num]==1)].tolist()\n",
    "list_var = [var + \"_\" + str(int_grid) + \"x\" + str(int_grid) + \".nc\" for var in list_var]\n",
    "\n",
    "\n",
    "def dataset_to_dataframe(ds, var_name, index_vars):\n",
    "    \"\"\"\n",
    "    Convert an xarray Dataset to a pandas DataFrame.\n",
    "    \n",
    "    :param ds: xarray Dataset to convert\n",
    "    :param var_name: the variable name to extract from the Dataset\n",
    "    :param index_vars: a list of variable names to use as index in the DataFrame\n",
    "    :return: pandas DataFrame with the specified variable and index\n",
    "    \"\"\"\n",
    "    df = ds[var_name].to_dataframe().reset_index()\n",
    "    df = df.set_index(index_vars)\n",
    "    return df\n",
    "\n",
    "def process_nc_dir_one(nc_dir_one, list_var, df_geo, int_grid):\n",
    "    for filename in os.listdir(nc_dir_one):\n",
    "        if filename in list_var:\n",
    "            with xr.open_dataset(os.path.join(nc_dir_one, filename)) as ds:\n",
    "                var_name = [var for var in ds.data_vars][0]\n",
    "                column_name = filename.replace(\"_\" + str(int_grid) + \"x\" + str(int_grid) + \".nc\", \"\")\n",
    "                # Convert the Dataset to a DataFrame\n",
    "                df_nc = dataset_to_dataframe(ds, var_name, ['lon', 'lat'])\n",
    "                # Merge the DataFrame with df_geo\n",
    "                df_geo = df_geo.merge(df_nc, left_on=['lon_grid', 'lat_grid'], right_index=True, how='left')\n",
    "                df_geo = df_geo.rename(columns={var_name: column_name})\n",
    "    return df_geo\n",
    "\n",
    "def process_nc_dir_year(nc_dir_year, list_var, df_geo, int_grid):\n",
    "    for filename in os.listdir(nc_dir_year):\n",
    "        if filename in list_var:\n",
    "            with xr.open_dataset(os.path.join(nc_dir_year, filename)) as ds:\n",
    "                ds = ds.sortby('lon')\n",
    "                ds = ds.sortby('lat')\n",
    "                ds = ds.sortby('year')\n",
    "                var_name = [var for var in ds.data_vars][0]\n",
    "                column_name = filename.replace(\"_\" + str(int_grid) + \"x\" + str(int_grid) + \".nc\", \"\")\n",
    "                # Convert the Dataset to a DataFrame\n",
    "                df_nc = dataset_to_dataframe(ds, var_name, ['lon', 'lat', 'year'])\n",
    "                # Merge the DataFrame with df_geo\n",
    "                df_geo = df_geo.merge(df_nc, left_on=['lon_grid', 'lat_grid', 'year'], right_index=True, how='left')\n",
    "                df_geo = df_geo.rename(columns={var_name: column_name})\n",
    "    return df_geo\n",
    "\n",
    "\n",
    "for year in range(2000, 2021):\n",
    "    print(year)\n",
    "    df_geo = pd.read_csv(path_match + \"lat_lon.csv\")\n",
    "    df_geo = df_geo.drop_duplicates(subset=['lat_grid', 'lon_grid'])\n",
    "    df_geo['year'] = year\n",
    "    print(df_geo.columns)\n",
    "    print(df_geo.shape)\n",
    "    \n",
    "    df_geo = process_nc_dir_one(nc_dir_one,list_var, df_geo, int_grid)\n",
    "    df_geo = process_nc_dir_year(nc_dir_year,list_var, df_geo, int_grid)\n",
    "    if data_type == \"normalization\" or data_type == \"standardization\":\n",
    "        for column in df_geo.columns:\n",
    "            if column not in ['lat_grid', 'lon_grid', 'year', 'country_id']:\n",
    "                if df_geo[column].lt(0).any() or df_geo[column].gt(1).any() or df_geo[column].lt(1e-20).any():\n",
    "                    print(f'Column {column} has values out of range [0, 1] or less than 1e-20. Replacing these values with 0.')\n",
    "                    df_geo.loc[df_geo[column].lt(0) | df_geo[column].gt(1) | df_geo[column].lt(1e-20), column] = 0\n",
    "    df_geo.fillna(0, inplace=True)\n",
    "\n",
    "    def replace_nan_in_array(array):\n",
    "        return np.where(np.isnan(array), 0, array)\n",
    "\n",
    "    df_geo2 = df_geo.applymap(replace_nan_in_array)\n",
    "    df_geo2 = df_geo2.fillna(0)\n",
    "    df_geo2 = df_geo2.replace([\"nan\", \"NaN\", \"NAN\", \"\", None], 0)\n",
    "    # 如果上面两行替换不了<class 'numpy.ndarray'>，大概率是碰到数组了\n",
    "    # If you can't replace <class ‘numpy.ndarray’> with the two lines above, the odds are that you've run into the array\n",
    "    column_means = df_geo2.mean()\n",
    "    filtered_columns = column_means[(column_means > 1) | (column_means == 0)]\n",
    "    columns_list = filtered_columns.index.tolist()\n",
    "    print(columns_list)\n",
    "    df_geo2.to_csv(path_match_geo + 'geo_global_'+str(year)+'.csv', encoding='utf-8-sig', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file merging\n",
    "# 文件合并\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder_path = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part0_treat\\match\\geo'\n",
    "\n",
    "file_names = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "dfs = [pd.read_csv(os.path.join(folder_path, file)) for file in file_names]\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "output_file = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part0_treat\\match\\geo_global.csv'\n",
    "\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEO-DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the data related to the sampled coordinates first, for quick use later. \n",
    "# 先把与采样坐标相关的数据匹配出来，方面后续快速使用 \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# 读取csv文件\n",
    "\n",
    "df = pd.read_csv(path_pre + \"lr_grid.csv\")\n",
    "df2 = pd.read_csv(path_pre + \"sw_grid.csv\")\n",
    "# ——————————————————————————————————————————————————————————————————\n",
    "int_grid = 1\n",
    "data_type = \"raw\"\n",
    "# ——————————————————————————————————————————————————————————————————\n",
    "df.rename(columns={'time_year': 'year'}, inplace=True)\n",
    "\n",
    "df = df[['lat_grid', 'lon_grid', 'year']]\n",
    "df = df.drop_duplicates(subset=['lat_grid', 'lon_grid', 'year'])\n",
    "df2 = df2[['lat_grid', 'lon_grid', 'year']]\n",
    "df2 = df2.drop_duplicates(subset=['lat_grid', 'lon_grid', 'year'])\n",
    "\n",
    "df_geo = pd.concat([df,df2],axis=0)\n",
    "df_geo = df_geo.drop_duplicates(subset=['lat_grid', 'lon_grid', 'year'])\n",
    "\n",
    "print(df_geo.columns)\n",
    "print(df_geo.shape)\n",
    "\n",
    "\n",
    "nc_dir_year = path_var_data + str(int_grid) + \"\\\\year\"\n",
    "nc_dir_one = path_var_data + str(int_grid) + \"\\\\one\"\n",
    "# 处理one文件夹下的nc文件\n",
    "# Handling of nc files in the one folder\n",
    "for filename in os.listdir(nc_dir_one):\n",
    "    if filename.endswith(\".nc\"):\n",
    "        with xr.open_dataset(os.path.join(nc_dir_one, filename)) as ds:\n",
    "            var_name = [var for var in ds.data_vars][0]  \n",
    "            column_name = filename.replace(\"_\" + str(int_grid) + \"x\" + str(int_grid) + \".nc\", \"\") \n",
    "            matched_data = []\n",
    "            for row in df_geo.itertuples():\n",
    "                matched_data.append(ds[var_name].sel(lon=row.lon_grid, lat=row.lat_grid, method='nearest').values)\n",
    "            df_geo[column_name] = matched_data\n",
    "\n",
    "# 处理year文件夹下的nc文件\n",
    "# Handling of nc files in the year folder\n",
    "for filename in os.listdir(nc_dir_year):\n",
    "    if filename.endswith(\".nc\"):\n",
    "        print(filename)\n",
    "        with xr.open_dataset(os.path.join(nc_dir_year, filename)) as ds:\n",
    "            ds = ds.sortby('lon') \n",
    "            ds = ds.sortby('lat') \n",
    "            ds = ds.sortby('year') \n",
    "            var_name = [var for var in ds.data_vars][0]  \n",
    "            column_name = filename.replace(\"_\" + str(int_grid) + \"x\" + str(int_grid) + \".nc\", \"\")  \n",
    "\n",
    "            matched_data = []\n",
    "            for row in df_geo.itertuples():\n",
    "                matched_data.append(ds[var_name].sel(lon=row.lon_grid, lat=row.lat_grid, year=row.year, method='nearest').values)\n",
    "            df_geo[column_name] = matched_data\n",
    "\n",
    "\n",
    "df_geo.fillna(0, inplace=True)\n",
    "\n",
    "def replace_nan_in_array(array):\n",
    "    return np.where(np.isnan(array), 0, array)\n",
    "df_geo2 = df_geo.applymap(replace_nan_in_array)\n",
    "df_geo2 = df_geo2.fillna(0)\n",
    "df_geo2 = df_geo2.replace([\"nan\", \"NaN\", \"NAN\", \"\", None], 0)\n",
    "column_means = df_geo2.mean()\n",
    "filtered_columns = column_means[(column_means > 1) | (column_means == 0)]\n",
    "\n",
    "columns_list = filtered_columns.index.tolist()\n",
    "\n",
    "df_geo2.to_csv(path_match + 'geo_all.csv', encoding='utf-8-sig', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FISH-DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching geospatial variables to fish data\n",
    "# 为鱼类数据匹配地理空间变量\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import myfunction as mf\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(path_pre + \"lr_grid_avg.csv\")\n",
    "\n",
    "int_grid = 1\n",
    "\n",
    "data_type = \"raw\"\n",
    "\n",
    "df.rename(columns={'time_year': 'year'}, inplace=True)\n",
    "\n",
    "df = df.dropna(subset=['value'])\n",
    "\n",
    "dict_inf1 = {'pon':['poid']}\n",
    "df = mf.append_inf(df, dict_inf1)\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "posname_df = pd.read_csv(path_one_spdb + 'posname.csv')\n",
    "posname_df = posname_df.drop(columns=[\"50%\"])\n",
    "df = df.merge(posname_df, how='left', on='posname').drop('posname', axis=1).rename(columns={'id':'posname'})\n",
    "# ----------------------------------------------\n",
    "\n",
    "\n",
    "df = df[['lat_grid', 'lon_grid', 'habitat', 'organ', 'year', 'poid', \"spid\", \"value\",\"posname\"]]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df.shape)\n",
    "\n",
    "dict_inf1 = {\"po\":[\"po_carbon\",\"po_f_carbon\",\"po_classification\",\"po_chain\",\"po_m_w\",\"po_xlogp\",\"log_Koc\", \"log_Kow\",\"log_Kaw\",\n",
    "                    \"log_Koa_dry\",\"log_Koa_wet\",\"log_KHxd_air\",\"log_Koil_w\",\"log_Koil_air\",\"density\",\"melting_point\",\n",
    "                    \"boiling_point\",\"solubility\"]\n",
    "                    ,\"sp\":[\"length_last\",\"weight_last\",\"troph_last\",\"class\",\"order\",\"family\",\"genus\"]}\n",
    "df[\"spid\"] = df[\"spid\"].astype(int) \n",
    "df_imputer = mf.append_inf(df, dict_inf1)\n",
    "print(df_imputer.shape)\n",
    "\n",
    "df_imputer = df_imputer.drop(columns=[\"poid\"])\n",
    "# ----------------------------------------------\n",
    "if data_type == \"raw\":\n",
    "    class_df = pd.read_csv(path_one_spdb + 'class.csv')\n",
    "    class_df = class_df.drop(columns=[\"50%\"])\n",
    "\n",
    "    order_df = pd.read_csv(path_one_spdb + 'order.csv')\n",
    "    order_df = order_df.drop(columns=[\"50%\"])\n",
    "\n",
    "    family_df = pd.read_csv(path_one_spdb + 'family.csv')\n",
    "    family_df = family_df.drop(columns=[\"50%\"])\n",
    "\n",
    "\n",
    "    genus_df = pd.read_csv(path_one_spdb + 'genus.csv')\n",
    "    genus_df = genus_df.drop(columns=[\"50%\"])\n",
    "\n",
    "    df_imputer = df_imputer.merge(class_df, how='left', on='class').drop('class', axis=1).rename(columns={'id':'class'})\n",
    "    df_imputer = df_imputer.merge(family_df, how='left', on='family').drop('family', axis=1).rename(columns={'id':'family'})\n",
    "    df_imputer = df_imputer.merge(order_df, how='left', on='order').drop('order', axis=1).rename(columns={'id':'order'})\n",
    "    df_imputer = df_imputer.merge(genus_df, how='left', on='genus').drop('genus', axis=1).rename(columns={'id':'genus'})  \n",
    "\n",
    "    print(df_imputer[['length_last','weight_last','troph_last']].isnull().sum())\n",
    "    df_imputer.to_csv(path_match + \"lr_all_avg.csv\", index=False)\n",
    "\n",
    "df_all = pd.read_csv(path_match + \"lr_all_avg.csv\")\n",
    "print(df_all.shape)\n",
    "df_fish = df_all[df_all['class']==0]\n",
    "list_organ = ['liver','muscle','whole']\n",
    "df_fish = df_fish[df_fish['organ'].isin(list_organ)]\n",
    "print(df_fish.shape)\n",
    "df_fish.to_csv(path_match + \"lr_fish_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WATER-DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching geospatial variables to water data\n",
    "# 为水体数据匹配地理空间变量\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import myfunction as mf\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 读取csv文件\n",
    "\n",
    "df = pd.read_csv(path_pre + \"sws7_grid_1.csv\")\n",
    "# ---------------------------------------------------------------\n",
    "dict_inf1 = {'pon':['poid']}\n",
    "df = mf.append_inf(df, dict_inf1)\n",
    "# ----------------------------------------------\n",
    "\n",
    "posname_df = pd.read_csv(path_one_spdb + 'posname.csv')\n",
    "posname_df = posname_df.drop(columns=[\"50%\"])\n",
    "df = df.merge(posname_df, how='left', on='posname').drop('posname', axis=1).rename(columns={'id':'posname'})\n",
    "# ----------------------------------------------\n",
    "df = df[['lat_grid', 'lon_grid','year', 'poid',\"value\",\"posname\"]]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df.shape)\n",
    "# ----------------------------------------------\n",
    "\n",
    "dict_inf1 = {\"po\":[\"po_carbon\",\"po_f_carbon\",\"po_classification\",\"po_chain\",\"po_m_w\",\"po_xlogp\",\"log_Koc\", \"log_Kow\",\"log_Kaw\",\n",
    "                    \"log_Koa_dry\",\"log_Koa_wet\",\"log_KHxd_air\",\"log_Koil_w\",\"log_Koil_air\",\"density\",\"melting_point\",\n",
    "                    \"boiling_point\",\"solubility\"]}\n",
    "\n",
    "df_imputer = mf.append_inf(df, dict_inf1)\n",
    "\n",
    "df_imputer = df_imputer.drop(columns=[\"poid\"])\n",
    "# ----------------------------------------------\n",
    "\n",
    "df_imputer.to_csv(path_match + \"sw_s7_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FISH-WATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining spatial and temporal overlap data for water and fish\n",
    "# 获取水和鱼时空重叠数据\n",
    "df_sp = pd.read_csv(path_match + \"lr_fish_avg.csv\")\n",
    "df_sw = pd.read_csv(path_match + \"sw_s7_avg.csv\")\n",
    "print(\"sp-shape\", df_sp.shape)\n",
    "print(\"sw-shape\", df_sw.shape)\n",
    "\n",
    "\n",
    "df_sw = df_sw.rename(columns={'value': 'sw_value'})\n",
    "\n",
    "df_merged = pd.merge(df_sp, df_sw[['lat_grid', 'lon_grid', 'posname', 'year', 'sw_value']], on=['lat_grid', 'lon_grid', 'posname', 'year'], how='left')\n",
    "\n",
    "df_merged = df_merged[df_merged[\"sw_value\"].notna()]\n",
    "print(\"sw-sp\", df_merged.shape)\n",
    "print(mf.col_describe(df_merged,'posname'))\n",
    "\n",
    "df_merged.to_csv(path_match + \"lr_sws7_fish_avg.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
