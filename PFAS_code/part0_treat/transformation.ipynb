{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import myfunction as mf\n",
    "path_data_raw = \"C:/Users/dell/OneDrive/file/\"\n",
    "path_country_nc = \"C:/Users/dell/OneDrive/file/nc\"\n",
    "path_one_spdb = 'C:/Users/dell/OneDrive/file/SPDB/'\n",
    "drive_letter = 'E:'\n",
    "\n",
    "path_pre = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/pretreatment/\"\n",
    "path_match = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/match/\"\n",
    "path_semdata = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/semdata/\"\n",
    "path_temp = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/temp/\"\n",
    "path_split_lr = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/semdata/lr/\"\n",
    "path_split_sw = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/semdata/sw/\"\n",
    "path_var_data = drive_letter + \"/wyy/SPDB_database/data/raw/\"\n",
    "\n",
    "mark_num = \"25\"\n",
    "meta_name = \"meta_data.csv\"\n",
    "list_color = [\"#ee877c\", \"#8bd0e3\", \"#6abeae\", \"#808eaf\", \"#f7bba8\", \"#acb4cc\", \"#b5e0d5\", \"#e86462\", \"#a89687\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Preparation\n",
    "# 函数准备\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy import stats\n",
    "def preprocess(df_data, treat_method, list_var=None):\n",
    "    \"\"\"\n",
    "    sem uses raw uniformly and then uses this function to preprocess it. \n",
    "    normalisation: normalize the maximum and minimum values.\n",
    "    standardisation:Z-score normalization.\n",
    "    box_cox:box_cox transformation\n",
    "    others: normal-z-score non-normal-logarithmic\n",
    "    list_var is left out to process all columns, otherwise only the element columns are processed.\n",
    "    ---\n",
    "    sem统一使用raw 然后再使用这个函数去预处理 \n",
    "    normalization:最大值最小值归一化\n",
    "    standardization:Z-score标准化\n",
    "    box_cox:box_cox变换\n",
    "    其他：正态-z-score  非正-对数\n",
    "    list_var不填就是对所有列处理 否则只对元素列进行处理\n",
    "    \"\"\"\n",
    "    \n",
    "    if list_var is None:\n",
    "        list_var = df_data.columns\n",
    "    \n",
    "    if treat_method == \"normalization\":\n",
    "        df_data[list_var] = (df_data[list_var] - df_data[list_var].min()) / (df_data[list_var].max() - df_data[list_var].min())\n",
    "    elif treat_method == \"standardization\":\n",
    "        df_data[list_var] = (df_data[list_var] - df_data[list_var].mean()) / df_data[list_var].std()\n",
    "    elif treat_method == \"box_cox\":\n",
    "        numeric_cols = df_data[list_var].select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            df_data[col], _ = stats.boxcox(df_data[col])\n",
    "    elif treat_method == \"log\":\n",
    "        df_data[list_var] = np.log10(df_data[list_var])\n",
    "    elif treat_method == \"yeo_johnson\":\n",
    "        numeric_cols = df_data[list_var].select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            df_data[col], _ = stats.yeojohnson(df_data[col])\n",
    "    elif treat_method == \"auto\":\n",
    "        df_meta = pd.read_csv(path_data_raw + meta_name, encoding=\"utf-8\")\n",
    "        list_yj_meta = df_meta['var_name'][df_meta['var_type0']==1]\n",
    "        list_bc_meta = df_meta['var_name'][df_meta['var_type0']==0]\n",
    "        list_var = [item for item in list_var if 'log_' not in item]\n",
    "        list_yj = list(set(list_yj_meta) & set(list_var))\n",
    "        list_bc = list(set(list_bc_meta) & set(list_var))\n",
    "        print('YJ:',list_yj)\n",
    "        print('BC:',list_bc)\n",
    "\n",
    "        for col in list_yj:\n",
    "            try:\n",
    "                df_data[col], _ = stats.yeojohnson(df_data[col])\n",
    "            except:\n",
    "                print('error:',col)\n",
    "                pass\n",
    "        for col in list_bc:\n",
    "            try:\n",
    "                df_data[col], _ = stats.boxcox(df_data[col])\n",
    "            except:\n",
    "                print('error:',col)\n",
    "                pass\n",
    "    else:\n",
    "        # 没指定方法就默认先检查数据是否正态分布，正态跳过，非正态直接yeojohnson\n",
    "        # No method is specified, the default first check whether the data is normally distributed, \n",
    "        # normal skip, non-normal directly yeojohnson\n",
    "        for var in list_var:\n",
    "            p_val = stats.normaltest(df_data[var]).pvalue\n",
    "            if p_val > 0.05:\n",
    "                print('normal:',var)\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    df_data[var], _ = stats.yeojohnson(df_data[var])\n",
    "                except:\n",
    "                    print('error:',var)\n",
    "                    pass\n",
    "                \n",
    "    return df_data\n",
    "\n",
    "def count_variance(df_sem_data):\n",
    "    \"\"\"\n",
    "    Calculate all variables\n",
    "    ---\n",
    "    计算所有变量\n",
    "    \"\"\"\n",
    "    print(df_sem_data.shape)\n",
    "    mean = df_sem_data.mean()\n",
    "    std_dev = df_sem_data.std()\n",
    "    variance = df_sem_data.var()\n",
    "    df_results = pd.DataFrame({\n",
    "        'var': df_sem_data.columns,\n",
    "        'mean': mean.values,\n",
    "        'SD': std_dev.values,\n",
    "        'variance': variance.values\n",
    "    })\n",
    "\n",
    "    min_variance = df_results[\"variance\"].min()\n",
    "    max_variance = df_results[\"variance\"].max()\n",
    "    print(min_variance, max_variance)\n",
    "    print(max_variance/min_variance)\n",
    "    return df_results\n",
    "\n",
    "def convert_temp(df_o, from_scale, to_scale):\n",
    "    \"\"\"\"\n",
    "    Convert units of temperature variables, \n",
    "    C:Celsius, F:Fahrenheit, K:Kelvin, \n",
    "    now only supports variables with global coverage, \n",
    "    non-global zeros are also converted.\n",
    "    ---\n",
    "    转换温度变量的单位,\n",
    "    C:Celsius,F:Fahrenheit,K:Kelvin,\n",
    "    现在只支持覆盖全球的变量,\n",
    "    非全球的0也会被变换\n",
    "    \"\"\"\n",
    "    df = df_o.copy()\n",
    "    df_meta = pd.read_csv(path_data_raw + meta_name, encoding=\"utf-8\")\n",
    "    list_all_var = df_meta['var_name'][(df_meta['var_select' + mark_num]==1)&(df_meta['var_temp']==1)].to_list()\n",
    "    for col in list_all_var:\n",
    "        if from_scale == \"C\":\n",
    "            if to_scale == \"F\":\n",
    "                df[col] = (df[col] * 9/5) + 32\n",
    "            elif to_scale == \"K\":\n",
    "                df[col] = df[col] + 273.15\n",
    "        elif from_scale == \"F\":\n",
    "            if to_scale == \"C\":\n",
    "                df[col] = (df[col] - 32) * 5/9\n",
    "            elif to_scale == \"K\":\n",
    "                df[col] = ((df[col] - 32) * 5/9) + 273.15\n",
    "        elif from_scale == \"K\":\n",
    "            if to_scale == \"C\":\n",
    "                df[col] = df[col] - 273.15\n",
    "            elif to_scale == \"F\":\n",
    "                df[col] = ((df[col] - 273.15) * 9/5) + 32\n",
    "    return df\n",
    "\n",
    "def encode_categorical(df_train, list_train):\n",
    "    \"\"\"\n",
    "    Dummy coding of categorical variables\n",
    "    ---\n",
    "    对分类变量进行虚拟编码\n",
    "    \"\"\"\n",
    "    df_encoded = pd.get_dummies(df_train[list_train], drop_first=True)\n",
    "    list_var = df_encoded.columns.tolist()\n",
    "    df_train = df_train.drop(columns=list_train)\n",
    "    df_train = pd.concat([df_train, df_encoded], axis=1)\n",
    "    return df_train, list_var\n",
    "\n",
    "def get_var_str(df, list_train):\n",
    "    \"\"\"\n",
    "    Categorical variables are sequentially coded, \n",
    "    here inversely coded as text, \n",
    "    to facilitate virtual coding.\n",
    "    ---\n",
    "    分类变量被顺序编码  \n",
    "    这里逆编码为文本 \n",
    "    目的是为了方便虚拟编码\n",
    "    \"\"\"\n",
    "    for var_change in list_train:\n",
    "        var_df = pd.read_csv('E:/wyy/SPDB_database/data/spdb/' +var_change +'.csv')\n",
    "        var_df = var_df.drop(columns=[\"50%\"])\n",
    "        df = df.merge(var_df, left_on=var_change, right_on='id', how='left')\n",
    "        print(df.columns)\n",
    "        df = df.drop(columns=[var_change + '_x'])\n",
    "        df = df.rename(columns={var_change+'_y': var_change})\n",
    "        df = df.drop(columns=['id'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEO-DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data conversion of spatial geographic variables\n",
    "# 对空间地理变量进行数据转换\n",
    "df_data_raw = pd.read_csv(path_match + 'geo_global.csv')\n",
    "\n",
    "df_data_raw = df_data_raw[(df_data_raw['lon_grid'] != -180) & (df_data_raw['lat_grid'] != -90)]\n",
    "df_meta = pd.read_csv(path_data_raw + meta_name, encoding=\"utf-8\")\n",
    "df_data_raw = convert_temp(df_data_raw, \"K\", \"C\")\n",
    "\n",
    "list_meta = df_meta[\"var_name\"][df_meta[\"var_select\" + mark_num]==1].to_list()\n",
    "list_data = df_data_raw.columns.tolist()\n",
    "list_select = list(set(list_meta)&set(list_data))\n",
    "\n",
    "df_data1 = df_data_raw[list_select]\n",
    "second_treat = \"auto\"\n",
    "\n",
    "if second_treat == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    df_data1 = preprocess(df_data1, treat_method=second_treat)\n",
    "feature_type = \"standardization\"\n",
    "df_data1 = preprocess(df_data1, treat_method=feature_type)\n",
    "df_sem_variance = count_variance(df_data1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "treat_mark1 = \"au_\"\n",
    "treat_mark2 = \"to_\"\n",
    "\n",
    "df_sem_variance.to_csv(path_temp + \"variance_geo\"+mark_num+\".csv\", index=False)\n",
    "\n",
    "\n",
    "df_data = pd.concat([df_data1, df_data_raw[[\"lon_grid\", \"lat_grid\", 'year']]], axis=1)\n",
    "sem_data_name = \"geo_global_\"+ treat_mark1 + treat_mark2 + mark_num+\".csv\"\n",
    "print(sem_data_name)\n",
    "print(df_data.columns)\n",
    "df_data.to_csv(path_semdata + sem_data_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data conversion of fish data\n",
    "# 对鱼类数据进行数据转换\n",
    "\n",
    "data_type = \"fish\"\n",
    "df_data_raw = pd.read_csv(path_match + 'lr_' + data_type +'_avg.csv')\n",
    "df_data_raw = df_data_raw[(df_data_raw['length_last'].notna())&(df_data_raw['weight_last'].notna())&(df_data_raw['troph_last'].notna())]\n",
    "df_data_raw = df_data_raw.rename(columns={\"length_last\": \"sp_length\", 'weight_last': 'sp_weight', 'troph_last': 'sp_troph'})\n",
    "\n",
    "df_meta = pd.read_csv(path_data_raw + meta_name, encoding=\"utf-8\")\n",
    "list_meta = df_meta[\"var_name\"][df_meta[\"var_select\" + mark_num]==1].to_list()\n",
    "# ------------------------------------------------------------------\n",
    "df_features = df_data_raw.drop(columns=\"value\")\n",
    "list_categorical = [\"organ\",'habitat']\n",
    "for var in list_categorical:\n",
    "    df_features = df_data_raw.drop(columns=var)\n",
    "list_fea = df_features.columns.tolist()\n",
    "list_select = list(set(list_fea)&set(list_meta))\n",
    "df_data1 = df_features[list_select]\n",
    "\n",
    "list_categorical_t = [\"po_chain\"]\n",
    "for cat in list_categorical_t:\n",
    "    if \"po_chain\" in df_data1.columns:\n",
    "        df_data1 = df_data1.drop(columns=\"po_chain\")\n",
    "\n",
    "print(df_data1.shape)\n",
    "negative_treat = \"N\"\n",
    "second_treat = \"auto\"\n",
    "if second_treat == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    df_data1 = preprocess(df_data1, treat_method=second_treat)\n",
    "feature_type = \"standardization\"\n",
    "df_data2 = preprocess(df_data1, treat_method=feature_type)\n",
    "\n",
    "df_data2 = pd.concat([df_data2, df_data_raw[\"value\"]], axis=1)\n",
    "if second_treat == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    # box_cox, yeo_johnson\n",
    "    value_treat = \"box_cox\"\n",
    "    df_data2 = preprocess(df_data2, value_treat, [\"value\"])\n",
    "value_treat2 = \"standardization\"\n",
    "df_data2 = preprocess(df_data2, value_treat2, [\"value\"])\n",
    "df_sem_variance = count_variance(df_data2)\n",
    "\n",
    "df_data3 = df_data_raw[list_categorical]\n",
    "\n",
    "print(df_data3.head())\n",
    "df_data4, list_new_categorical = encode_categorical(df_data3, list_categorical)\n",
    "\n",
    "df_data5 = df_data_raw[list_categorical_t]\n",
    "# ------------------------------------------------------------------\n",
    "treat_mark1 = \"au_\"\n",
    "treat_mark2 = \"to_\"\n",
    "\n",
    "df_sem_variance.to_csv(path_temp + \"variance_lr\"+data_type+\".csv\", index=False)\n",
    "\n",
    "df_data = pd.concat([df_data2, df_data4, df_data5], axis=1)\n",
    "df_data = pd.concat([df_data, df_data_raw[[\"lon_grid\", \"lat_grid\", 'year']]], axis=1)\n",
    "sem_data_name = \"sem_lr_\" +data_type+\"_\"+ treat_mark1 + treat_mark2 + mark_num+\"_avg.csv\"\n",
    "\n",
    "colnames = df_data.columns.tolist()\n",
    "habitat_columns = [col for col in colnames if 'habitat' in col]\n",
    "print(habitat_columns)\n",
    "print(df_data.shape)\n",
    "if len(habitat_columns) == 1:\n",
    "    df_data = df_data.rename(columns={habitat_columns[0]: 'habitat'})\n",
    "\n",
    "if second_treat == \"auto\":\n",
    "    df_geo = pd.read_csv(path_semdata + \"geo_global_\"+ treat_mark1 + treat_mark2 + mark_num+\".csv\")\n",
    "elif second_treat == \"N\" and feature_type == \"N\":\n",
    "    df_geo = pd.read_csv(path_match + \"geo_all.csv\")\n",
    "merged_df = pd.merge(df_geo, df_data, on=['lat_grid', 'lon_grid', 'year'],how='right')\n",
    "\n",
    "merged_df.to_csv(path_semdata + sem_data_name, index=False)\n",
    "print(merged_df.shape)\n",
    "print(sem_data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data conversion of water data\n",
    "# 对水体数据进行数据转换\n",
    "\n",
    "sw_mark = 's7'\n",
    "df_data_raw = pd.read_csv(path_match + 'sw_' + sw_mark + '_avg.csv')\n",
    "print(\"raw data shape\",df_data_raw.shape)\n",
    "df_meta = pd.read_csv(path_data_raw + meta_name, encoding=\"utf-8\")\n",
    "df_features = df_data_raw.drop(columns=\"value\")\n",
    "\n",
    "print(\"feature data shape\",df_features.shape)\n",
    "\n",
    "list_sp_var = [\"sp_length\", 'sp_weight', 'sp_troph']\n",
    "list_var_select = df_meta[\"var_name\"][df_meta[\"var_select\" + mark_num]==1].to_list()\n",
    "list_var_select = [var for var in list_var_select if var not in list_sp_var]\n",
    "list_fea = df_data_raw.columns.tolist()\n",
    "list_select = list(set(list_fea)&set(list_var_select))\n",
    "df_data1 = df_features[list_select]\n",
    "\n",
    "list_categorical_t = [\"po_chain\"]\n",
    "for cat in list_categorical_t:\n",
    "    if \"po_chain\" in df_data1.columns:\n",
    "        df_data1 = df_data1.drop(columns=\"po_chain\")\n",
    "\n",
    "negative_treat = \"N\"\n",
    "\n",
    "second_treat = \"auto\"\n",
    "\n",
    "if second_treat == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    df_data1 = preprocess(df_data1, treat_method=second_treat)\n",
    "print(\"data1 shape\",df_data1.shape)\n",
    "\n",
    "feature_type = \"standardization\"\n",
    "\n",
    "if feature_type == \"N\":\n",
    "    df_data2 = pd.concat([df_data1, df_data_raw[\"value\"]], axis=1)\n",
    "else:\n",
    "    df_data2 = preprocess(df_data1, treat_method=feature_type)\n",
    "    df_data2 = pd.concat([df_data2, df_data_raw[\"value\"]], axis=1)\n",
    "# ------------------------------------------------------------------\n",
    "if second_treat == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    value_treat = \"box_cox\"\n",
    "    df_data2 = preprocess(df_data2, value_treat, [\"value\"])\n",
    "if feature_type == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    df_data2 = preprocess(df_data2, feature_type, [\"value\"])\n",
    "    df_sem_variance = count_variance(df_data2)\n",
    "    print(\"data2 shape\",df_data2.shape)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "df_data5 = df_data_raw[list_categorical_t]\n",
    "\n",
    "print(\"data5 shape\",df_data5.shape)\n",
    "\n",
    "if second_treat == \"auto\":\n",
    "    treat_mark1 = \"au_\"\n",
    "    treat_mark2 = \"to_\"\n",
    "elif second_treat == \"N\" and feature_type == \"N\":\n",
    "    treat_mark1 = \"raw_\"\n",
    "    treat_mark2 = \"raw_\"\n",
    "elif second_treat == \"N\" and feature_type == \"standardization\":\n",
    "    treat_mark1 = \"raw_\"\n",
    "    treat_mark2 = \"std_\"\n",
    "\n",
    "df_data = pd.concat([df_data2, df_data5], axis=1)\n",
    "df_data = pd.concat([df_data, df_data_raw[[\"lon_grid\", \"lat_grid\", 'year']]], axis=1)\n",
    "\n",
    "sem_data_name = \"sem_sw_\" + sw_mark + \"_\" + treat_mark1 + treat_mark2 + mark_num+\"_avg.csv\"\n",
    "df_sem_variance.to_csv(path_temp +\"variance_sw_\" + sw_mark + \".csv\", index=False)\n",
    "\n",
    "if second_treat == \"auto\":\n",
    "    df_geo = pd.read_csv(path_semdata + \"geo_global_\"+ treat_mark1 + treat_mark2 + mark_num+\".csv\")\n",
    "elif second_treat == \"N\" and feature_type == \"N\":\n",
    "    df_geo = pd.read_csv(path_match + \"geo_all.csv\")\n",
    "\n",
    "merged_df = pd.merge(df_geo, df_data, on=['lat_grid', 'lon_grid', 'year'],how='right')\n",
    "\n",
    "merged_df.to_csv(path_semdata + sem_data_name, index=False)\n",
    "\n",
    "merged_df.to_csv(path_semdata + sem_data_name, index=False)\n",
    "print(\"merge shape\",merged_df.shape)\n",
    "print(sem_data_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WATER-FISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data conversion of spatial and temporal overlap data for water and fish\n",
    "# 对水和鱼时空重叠数据进行转换\n",
    "\n",
    "data_type = \"fish\"\n",
    "df_data_raw = pd.read_csv(path_match + 'lr_sws7_' + data_type +'_avg.csv')\n",
    "\n",
    "df_data_raw = df_data_raw[(df_data_raw['length_last'].notna())&(df_data_raw['weight_last'].notna())&(df_data_raw['troph_last'].notna())]\n",
    "df_data_raw = df_data_raw.rename(columns={\"length_last\": \"sp_length\", 'weight_last': 'sp_weight', 'troph_last': 'sp_troph'})\n",
    "df_meta = pd.read_csv(path_data_raw + meta_name, encoding=\"utf-8\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "df_features = df_data_raw.drop(columns=\"value\")\n",
    "df_features = df_data_raw.drop(columns=\"sw_value\")\n",
    "\n",
    "list_categorical = [\"organ\",'habitat']\n",
    "for var in list_categorical:\n",
    "    df_features = df_data_raw.drop(columns=var)\n",
    "\n",
    "list_fea = df_features.columns.tolist()\n",
    "list_meta = df_meta[\"var_name\"][df_meta[\"var_select\" + mark_num]==1].to_list()\n",
    "list_select = list(set(list_fea)&set(list_meta))\n",
    "\n",
    "df_data1 = df_features[list_select]\n",
    "# ------------------------------------------------------------------\n",
    "list_categorical_t = [\"po_chain\"]\n",
    "for cat in list_categorical_t:\n",
    "    if \"po_chain\" in df_data1.columns:\n",
    "        df_data1 = df_data1.drop(columns=\"po_chain\")\n",
    "\n",
    "print(df_data1.shape)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "negative_treat = \"N\"\n",
    "# ------------------------------------------------------------------\n",
    "# normalization, standardization, box_cox\n",
    "second_treat = \"auto\"\n",
    "\n",
    "if second_treat == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    df_data1 = preprocess(df_data1, treat_method=second_treat)\n",
    "\n",
    "\n",
    "feature_type = \"standardization\"\n",
    "df_data2 = preprocess(df_data1, treat_method=feature_type)\n",
    "# ------------------------------------------------------------------\n",
    "df_data2 = pd.concat([df_data2, df_data_raw[[\"value\",'sw_value']]], axis=1)\n",
    "if second_treat == \"N\":\n",
    "    pass\n",
    "else:\n",
    "    value_treat = \"box_cox\"\n",
    "    df_data2 = preprocess(df_data2, value_treat, [\"value\",'sw_value'])\n",
    "value_treat2 = \"standardization\"\n",
    "df_data2 = preprocess(df_data2, value_treat2, [\"value\",'sw_value'])\n",
    "df_sem_variance = count_variance(df_data2)\n",
    "# ------------------------------------------------------------------\n",
    "df_data3 = df_data_raw[list_categorical]\n",
    "print(df_data3.head())\n",
    "df_data4, list_new_categorical = encode_categorical(df_data3, list_categorical)\n",
    "# ------------------------------------------------------------------\n",
    "df_data5 = df_data_raw[list_categorical_t]\n",
    "\n",
    "treat_mark1 = \"au_\"\n",
    "treat_mark2 = \"to_\"\n",
    "\n",
    "df_sem_variance.to_csv(path_temp + \"variance_lr_sws7_\"+data_type+\"_avg.csv\", index=False)\n",
    "\n",
    "df_data = pd.concat([df_data2, df_data4, df_data5], axis=1)\n",
    "df_data = pd.concat([df_data, df_data_raw[[\"lon_grid\", \"lat_grid\", 'year']]], axis=1)\n",
    "\n",
    "sem_data_name = \"sem_lr_sws7_\"+ treat_mark1 + treat_mark2 + mark_num+\"_avg.csv\"\n",
    "\n",
    "colnames = df_data.columns.tolist()\n",
    "habitat_columns = [col for col in colnames if 'habitat' in col]\n",
    "print(habitat_columns)\n",
    "print(df_data.shape)\n",
    "\n",
    "if len(habitat_columns) == 1:\n",
    "    df_data = df_data.rename(columns={habitat_columns[0]: 'habitat'})\n",
    "if second_treat == \"auto\":\n",
    "    df_geo = pd.read_csv(path_semdata + \"geo_global_\"+ treat_mark1 + treat_mark2 + mark_num+\".csv\")\n",
    "elif second_treat == \"N\" and feature_type == \"N\":\n",
    "    df_geo = pd.read_csv(path_match + \"geo_all.csv\")\n",
    "\n",
    "merged_df = pd.merge(df_geo, df_data, on=['lat_grid', 'lon_grid', 'year'],how='right')\n",
    "merged_df.to_csv(path_semdata + sem_data_name, index=False)\n",
    "print(merged_df.shape)\n",
    "print(sem_data_name)\n",
    "\n",
    "# Data are grouped according to time and space\n",
    "#数据根据时空进行分组\n",
    "treat_mark1 = \"au_\"\n",
    "treat_mark2 = \"to_\"\n",
    "df = pd.read_csv(path_semdata + \"sem_lr_sws7_\" + treat_mark1 + treat_mark2 + mark_num+\"_avg.csv\")\n",
    "df['cluster'] = df['lon_grid'].astype(str) + '&' + df['lat_grid'].astype(str) + '&' + df['year'].astype(str)\n",
    "cluster_dict = {cluster: i for i, cluster in enumerate(df['cluster'].unique())}\n",
    "df['cluster'] = df['cluster'].map(cluster_dict)\n",
    "cluster_counts = df['cluster'].value_counts()\n",
    "total_clusters = len(cluster_counts)\n",
    "\n",
    "df.to_csv(path_semdata + \"sem_lr_sws7_\" + treat_mark1 + treat_mark2 + mark_num + \"_cluster.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
