{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate THQ (Monte Carlo) \n",
    "\n",
    "计算THQ(蒙特卡洛)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import myfunction as mf\n",
    "path_data_raw = \"C:/Users/dell/OneDrive/file/\"\n",
    "path_country_nc = \"C:/Users/dell/OneDrive/file/nc\"\n",
    "path_country_csv = \"C:/Users/dell/OneDrive/file/csv/\"\n",
    "path_one_spdb = 'C:/Users/dell/OneDrive/file/SPDB/'\n",
    "drive_letter = 'E:'\n",
    "\n",
    "path_pre = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/pretreatment/\"\n",
    "path_match = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/match/\"\n",
    "path_semdata = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/semdata/\"\n",
    "\n",
    "\n",
    "path_2_preanalysis_data = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part2_sem/preanalysis/\"\n",
    "path_2_preanalysis_fig = drive_letter + \"/wyy/code_project/running_outcome/final_fig/SPDB/part2_sem/preanalysis/\"\n",
    "\n",
    "\n",
    "path_3_sw_forecast = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part3_forecast/sw_forecast/\"\n",
    "path_temp = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/temp/\"\n",
    "\n",
    "path_4_risk_assess = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part4_assess/\"\n",
    "path_4_risk_assess_grid = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part4_assess/grid/\"\n",
    "\n",
    "path_1_describe_fig_fish = drive_letter + \"/wyy/code_project/running_outcome/final_fig/SPDB/part1_describe/fish_diff/\"\n",
    "path_1_describe_global_map = drive_letter + \"/wyy/code_project/running_outcome/final_fig/SPDB/part1_describe/global_map/\"\n",
    "\n",
    "mark_num = \"25\"\n",
    "meta_name = \"meta_data.csv\"\n",
    "\n",
    "list_pfas =['PFOA', 'PFNA', 'PFDA', 'PFUnDA','PFDoDA','PFTrDA', 'PFTeDA', 'PFHxS', 'PFOS', 'FOSA', 'PFBA', 'PFPeA', 'PFHxA', 'PFHpA','PFBS']\n",
    "list_pfas_lc = ['PFOA', 'PFNA', 'PFDA', 'PFUnDA','PFDoDA','PFTrDA', 'PFTeDA', 'PFHxS', 'PFOS', 'FOSA']\n",
    "list_pfas_sc = ['PFBA', 'PFPeA', 'PFHxA', 'PFHpA','PFBS']\n",
    "list_color = [\"#4d8cbf\", \"#4f9c8b\", \"#555c6c\", \"#d77563\", \"#7d84a8\", \"#84aeb8\", \"#c3473b\", \"#89756d\",\"#ffb3cc\",\"#9a7ebf\",\"#ffddb8\", \"#c4eaff\", \"#d1c6ff\", \"#c2ffbf\", \"#f5f5b0\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm, triang\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "def truncated_normal(mean, cv, lower, upper):\n",
    "    std = mean * cv\n",
    "    a, b = (lower - mean) / std, (upper - mean) / std\n",
    "    return truncnorm(a, b, loc=mean, scale=std)\n",
    "\n",
    "def generate_consume(consume_series, cv=0.05):\n",
    "    result = np.zeros(len(consume_series))\n",
    "    non_zero_mask = consume_series > 0\n",
    "    \n",
    "    if non_zero_mask.any():\n",
    "        non_zero_values = consume_series[non_zero_mask]\n",
    "        generated_values = truncated_normal(\n",
    "            non_zero_values, \n",
    "            cv, \n",
    "            non_zero_values * 0.8, \n",
    "            non_zero_values * 1.2\n",
    "        ).rvs()\n",
    "        result[non_zero_mask] = generated_values\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calculate_thq(tc, fir, ef, ed, rfd, bw, at):\n",
    "    return (tc * fir * ef * ed) / (rfd * bw * at) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分析完成！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_weight = pd.read_csv(path_4_risk_assess + 'hi/weight.csv')\n",
    "df_life = pd.read_csv(path_4_risk_assess + 'hi/life_ex.csv')\n",
    "df_rfd = pd.read_csv(path_4_risk_assess + 'hi/rfd.csv')\n",
    "\n",
    "pfas_list = ['PFBS', 'PFDA', 'PFDoDA', 'PFHpA', 'PFHxA', 'PFHxS', 'PFNA', 'PFOA',\n",
    "               'PFOS', 'PFPeA', 'PFTeDA', 'PFTrDA', 'PFUnDA']\n",
    "n_simulations = 1000\n",
    "\n",
    "source = 'lr'\n",
    "for pfas in pfas_list:\n",
    "    df_pfas = pd.read_csv(path_4_risk_assess + f'pfas/{source}_{pfas}.csv')\n",
    "    df_pfas = df_pfas[df_pfas[pfas + '_min'].notna()]\n",
    "    df_pfas = df_pfas.merge(df_weight, on='country_id', how='left')\n",
    "    df_pfas = df_pfas.merge(df_life, on=['country_id', 'year'], how='left')\n",
    "    rfd_data = df_rfd[df_rfd['PFAS'] == pfas].iloc[0]\n",
    "    results = df_pfas[['lon', 'lat', 'year']].copy()\n",
    "\n",
    "    for i in range(n_simulations):\n",
    "        tc_column = np.random.choice([f'{pfas}_min', f'{pfas}_max', f'{pfas}_median'])\n",
    "        tc = df_pfas[tc_column]/1000\n",
    "        if rfd_data['min'] == rfd_data['max'] == rfd_data['median']:\n",
    "            rfd = rfd_data['median'] / 1000000\n",
    "        else:\n",
    "            min_val = rfd_data['min'] / 1000000\n",
    "            max_val = rfd_data['max'] / 1000000\n",
    "            median_val = rfd_data['median'] / 1000000\n",
    "            c = (median_val - min_val) / (max_val - min_val)\n",
    "            loc = min_val\n",
    "            scale = max_val - min_val\n",
    "            \n",
    "            rfd = triang.rvs(c, loc=loc, scale=scale)\n",
    "        bw = truncated_normal(df_pfas['weight'], 0.05, df_pfas['weight'] * 0.8, df_pfas['weight'] * 1.2).rvs()\n",
    "        life_ex = truncated_normal(df_pfas['life_ex'], 0.05, df_pfas['life_ex'] * 0.8, df_pfas['life_ex'] * 1.2).rvs()\n",
    "\n",
    "        ff_consume = generate_consume(df_pfas['ff_consume'])\n",
    "        sf_consume = generate_consume(df_pfas['sf_consume'])\n",
    "        ef = 365\n",
    "        at = 365 * life_ex\n",
    "        \n",
    "        thq_ff = calculate_thq(tc, ff_consume, ef, life_ex, rfd, bw, at)\n",
    "        thq_sf = calculate_thq(tc, sf_consume, ef, life_ex, rfd, bw, at)\n",
    "        thq_fish = thq_ff + thq_sf\n",
    "        results[f'thq_fish_{i}'] = thq_fish*0.71\n",
    "    output_path = path_4_risk_assess + f'results/{source}_{pfas}_thq.csv'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    results.to_csv(output_path, index=False)\n",
    "print(\"Over！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分析完成！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_weight = pd.read_csv(path_4_risk_assess + 'hi/weight.csv')\n",
    "df_life = pd.read_csv(path_4_risk_assess + 'hi/life_ex.csv')\n",
    "df_rfd = pd.read_csv(path_4_risk_assess + 'hi/rfd.csv')\n",
    "pfas_list = ['FOSA', 'PFBA', 'PFBS', 'PFDA', 'PFDoDA', 'PFHpA', 'PFHxA', 'PFHxS', 'PFNA', 'PFOA',\n",
    "              'PFOS', 'PFPeA', 'PFTeDA', 'PFTrDA', 'PFUnDA']\n",
    "n_simulations = 1000\n",
    "\n",
    "source = 'sw'\n",
    "for pfas in pfas_list:\n",
    "    df_pfas = pd.read_csv(path_4_risk_assess + f'pfas/{source}_{pfas}.csv')\n",
    "    df_pfas = df_pfas[df_pfas[pfas + '_min'].notna()]\n",
    "    df_pfas = df_pfas.merge(df_weight, on='country_id', how='left')\n",
    "    df_pfas = df_pfas.merge(df_life, on=['country_id', 'year'], how='left')\n",
    "    df_pfas['sw_consume'] = 2*0.2*0.5\n",
    "    rfd_data = df_rfd[df_rfd['PFAS'] == pfas].iloc[0]\n",
    "    results = df_pfas[['lon', 'lat', 'year']].copy()\n",
    "    for i in range(n_simulations):\n",
    "        tc_column = np.random.choice([f'{pfas}_min', f'{pfas}_max', f'{pfas}_median'])\n",
    "        tc = df_pfas[tc_column]/1000\n",
    "        if rfd_data['min'] == rfd_data['max'] == rfd_data['median']:\n",
    "            rfd = rfd_data['median'] / 1000000\n",
    "        else:\n",
    "            min_val = rfd_data['min'] / 1000000\n",
    "            max_val = rfd_data['max'] / 1000000\n",
    "            median_val = rfd_data['median'] / 1000000\n",
    "            c = (median_val - min_val) / (max_val - min_val)\n",
    "            loc = min_val\n",
    "            scale = max_val - min_val\n",
    "            \n",
    "            rfd = triang.rvs(c, loc=loc, scale=scale)\n",
    "        bw = truncated_normal(df_pfas['weight'], 0.05, df_pfas['weight'] * 0.8, df_pfas['weight'] * 1.2).rvs()\n",
    "        life_ex = truncated_normal(df_pfas['life_ex'], 0.05, df_pfas['life_ex'] * 0.8, df_pfas['life_ex'] * 1.2).rvs()\n",
    "\n",
    "        sw_consume  = generate_consume(df_pfas['sw_consume'])\n",
    "\n",
    "        ef = 365\n",
    "        at = 365 * life_ex\n",
    "\n",
    "        thq_water = calculate_thq(tc, sw_consume, ef, life_ex, rfd, bw, at)\n",
    "        results[f'thq_water_{i}'] = thq_water\n",
    "    output_path = path_4_risk_assess + f'results/{source}_{pfas}_thq.csv'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    results.to_csv(output_path, index=False)\n",
    "print(\"Over！\")\n",
    "# 115min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "Merged file saved: E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq\\lr_merged_thq.csv\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "Merged file saved: E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq\\sw_merged_thq.csv\n",
      "All files have been processed and merged.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "input_path = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\results'\n",
    "output_path = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "middle_parts = ['FOSA', 'PFBA', 'PFBS', 'PFDA', 'PFDoDA', 'PFHpA', 'PFHxA', 'PFHxS', 'PFNA', 'PFOA', 'PFOS', 'PFPeA', 'PFTeDA', 'PFTrDA', 'PFUnDA']\n",
    "\n",
    "for prefix in ['lr_', 'sw_']:\n",
    "    merged_df = None\n",
    "    \n",
    "    for middle in middle_parts:\n",
    "        file_name = f\"{prefix}{middle}_thq.csv\"\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df[['lon', 'lat', 'year']].head(5))\n",
    "        if merged_df is None:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df.iloc[:, 3:] += df.iloc[:, 3:]\n",
    "        del df\n",
    "    output_file = os.path.join(output_path, f\"{prefix}merged_thq.csv\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Merged file saved: {output_file}\")\n",
    "\n",
    "print(\"All files have been processed and merged.\")\n",
    "\n",
    "# 总共40min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "Merged file saved: E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq\\lr_merged_thq.csv\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "    lon   lat  year\n",
      "0 -69.0 -55.0  2000\n",
      "1 -68.0 -55.0  2000\n",
      "2 -71.0 -54.0  2000\n",
      "3 -68.0 -54.0  2000\n",
      "4 -69.0 -53.0  2000\n",
      "Merged file saved: E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq\\sw_merged_thq.csv\n",
      "All files have been processed and merged.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "input_path = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\results'\n",
    "output_path = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "middle_parts = ['FOSA', 'PFBA', 'PFBS', 'PFDA', 'PFDoDA', 'PFHpA', 'PFHxA', 'PFHxS', 'PFNA', 'PFOA', 'PFOS', 'PFPeA', 'PFTeDA', 'PFTrDA', 'PFUnDA']\n",
    "for prefix in ['lr_', 'sw_']:\n",
    "    merged_df = None\n",
    "    \n",
    "    for middle in middle_parts:\n",
    "        file_name = f\"{prefix}{middle}_thq.csv\"\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df[['lon', 'lat', 'year']].head(5))\n",
    "        if merged_df is None:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df.iloc[:, 3:] += df.iloc[:, 3:]\n",
    "        del df\n",
    "    \n",
    "    output_file = os.path.join(output_path, f\"{prefix}merged_thq.csv\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Merged file saved: {output_file}\")\n",
    "\n",
    "print(\"All files have been processed and merged.\")\n",
    "\n",
    "# 总共40min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lon             float64\n",
      "lat             float64\n",
      "year              int64\n",
      "thq_fish_0      float64\n",
      "thq_fish_1      float64\n",
      "                 ...   \n",
      "thq_fish_995    float64\n",
      "thq_fish_996    float64\n",
      "thq_fish_997    float64\n",
      "thq_fish_998    float64\n",
      "thq_fish_999    float64\n",
      "Length: 1003, dtype: object\n",
      "lon              float64\n",
      "lat              float64\n",
      "year               int64\n",
      "thq_water_0      float64\n",
      "thq_water_1      float64\n",
      "                  ...   \n",
      "thq_water_995    float64\n",
      "thq_water_996    float64\n",
      "thq_water_997    float64\n",
      "thq_water_998    float64\n",
      "thq_water_999    float64\n",
      "Length: 1003, dtype: object\n"
     ]
    }
   ],
   "source": [
    "input_path = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq'\n",
    "file_name1 = 'lr_merged_thq.csv'\n",
    "file_name2 = 'sw_merged_thq.csv'\n",
    "df_lr = pd.read_csv(os.path.join(input_path, file_name1))\n",
    "print(df_lr.dtypes)\n",
    "df_sw = pd.read_csv(os.path.join(input_path, file_name2))\n",
    "print(df_sw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq\\merged_thq.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_path = r'E:\\wyy\\code_project\\running_outcome\\final_data\\SPDB\\part4_assess\\thq'\n",
    "file_name1 = 'lr_merged_thq.csv'\n",
    "file_name2 = 'sw_merged_thq.csv'\n",
    "\n",
    "df_lr = pd.read_csv(os.path.join(input_path, file_name1))\n",
    "df_sw = pd.read_csv(os.path.join(input_path, file_name2))\n",
    "\n",
    "assert (df_lr[['lon', 'lat', 'year']] == df_sw[['lon', 'lat', 'year']]).all().all(), \"lon, lat, year columns do not match\"\n",
    "df_merged = df_lr[['lon', 'lat', 'year']]\n",
    "\n",
    "for i in range(1000):\n",
    "    df_merged[f'thq_{i}'] = df_lr[f'thq_fish_{i}'] + df_sw[f'thq_water_{i}']\n",
    "\n",
    "output_file = 'merged_thq.csv'\n",
    "df_merged.to_csv(os.path.join(input_path, output_file), index=False)\n",
    "\n",
    "print(f\"Merged data saved to {os.path.join(input_path, output_file)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
