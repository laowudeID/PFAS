{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate the data needed to calculate THQ\n",
    "\n",
    "整理计算THQ需要的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import myfunction as mf\n",
    "path_data_raw = \"C:/Users/dell/OneDrive/file/\"\n",
    "path_country_nc = \"C:/Users/dell/OneDrive/file/nc\"\n",
    "path_country_csv = \"C:/Users/dell/OneDrive/file/csv/\"\n",
    "path_one_spdb = 'C:/Users/dell/OneDrive/file/SPDB/'\n",
    "drive_letter = 'E:'\n",
    "\n",
    "path_pre = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/pretreatment/\"\n",
    "path_match = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/match/\"\n",
    "path_semdata = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part0_treat/semdata/\"\n",
    "\n",
    "\n",
    "path_2_preanalysis_data = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part2_sem/preanalysis/\"\n",
    "path_2_preanalysis_fig = drive_letter + \"/wyy/code_project/running_outcome/final_fig/SPDB/part2_sem/preanalysis/\"\n",
    "\n",
    "\n",
    "path_3_sw_forecast = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part3_forecast/sw_forecast/\"\n",
    "path_temp = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/temp/\"\n",
    "\n",
    "path_4_risk_assess = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part4_assess/\"\n",
    "path_4_risk_assess_grid = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part4_assess/grid/\"\n",
    "\n",
    "path_1_describe_fig_fish = drive_letter + \"/wyy/code_project/running_outcome/final_fig/SPDB/part1_describe/fish_diff/\"\n",
    "path_1_describe_global_map = drive_letter + \"/wyy/code_project/running_outcome/final_fig/SPDB/part1_describe/global_map/\"\n",
    "\n",
    "mark_num = \"25\"\n",
    "meta_name = \"meta_data.csv\"\n",
    "\n",
    "list_pfas =['PFOA', 'PFNA', 'PFDA', 'PFUnDA','PFDoDA','PFTrDA', 'PFTeDA', 'PFHxS', 'PFOS', 'FOSA', 'PFBA', 'PFPeA', 'PFHxA', 'PFHpA','PFBS']\n",
    "list_pfas_lc = ['PFOA', 'PFNA', 'PFDA', 'PFUnDA','PFDoDA','PFTrDA', 'PFTeDA', 'PFHxS', 'PFOS', 'FOSA']\n",
    "list_pfas_sc = ['PFBA', 'PFPeA', 'PFHxA', 'PFHpA','PFBS']\n",
    "list_color = [\"#4d8cbf\", \"#4f9c8b\", \"#555c6c\", \"#d77563\", \"#7d84a8\", \"#84aeb8\", \"#c3473b\", \"#89756d\",\"#ffb3cc\",\"#9a7ebf\",\"#ffddb8\", \"#c4eaff\", \"#d1c6ff\", \"#c2ffbf\", \"#f5f5b0\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight\n",
    "df_country = pd.read_excel(path_country_csv + \"hi_raw.xlsx\",sheet_name='country')\n",
    "dic_code = df_country.set_index(\"country_ename\")[\"country_id\"].to_dict()\n",
    "\n",
    "df_weight = pd.read_excel(path_country_csv + \"hi_raw.xlsx\",sheet_name='bmi')\n",
    "df_weight[\"country_id\"] = df_weight[\"country_ename\"].map(dic_code)\n",
    "df_weight = df_weight[['country_id','weight']]\n",
    "\n",
    "weight_mean = df_weight['weight'].mean()\n",
    "\n",
    "all_country_ids = df_country['country_id'].unique()\n",
    "existing_country_ids = df_weight['country_id'].unique()\n",
    "\n",
    "missing_country_ids = set(all_country_ids) - set(existing_country_ids)\n",
    "\n",
    "missing_data = pd.DataFrame({'country_id': list(missing_country_ids), 'weight': weight_mean})\n",
    "df_weight = pd.concat([df_weight, missing_data])\n",
    "df_weight = df_weight[(df_weight['country_id'].notna())&(df_weight['country_id']!=0)]\n",
    "\n",
    "df_weight.to_csv(path_4_risk_assess + 'hi/weight.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# life_ex\n",
    "df_life = pd.read_excel(path_country_csv + \"hi_raw.xlsx\",sheet_name='life')\n",
    "df_life = df_life[(df_life['Year']>1999)&(df_life['Year']<2021)]\n",
    "\n",
    "df_avg_life = df_life[['country_ename', \"life_ex\", 'Year']]\n",
    "df_avg_life = df_avg_life.rename(columns={'Year':'year'})\n",
    "df_country = pd.read_excel(path_country_csv + \"hi_raw.xlsx\",sheet_name='country')\n",
    "dic_code = df_country.set_index(\"country_ename\")[\"country_id\"].to_dict()\n",
    "\n",
    "df_avg_life[\"country_id\"] = df_avg_life[\"country_ename\"].map(dic_code)\n",
    "df_avg_life = df_avg_life[['country_id','life_ex', 'year']]\n",
    "\n",
    "all_country_ids = df_country['country_id'].unique()\n",
    "existing_country_ids = df_avg_life['country_id'].unique()\n",
    "missing_country_ids = set(all_country_ids) - set(existing_country_ids)\n",
    "all_years = df_avg_life['year'].unique()\n",
    "yearly_life_mean = df_avg_life.groupby('year')['life_ex'].mean()\n",
    "missing_data = []\n",
    "for country_id in missing_country_ids:\n",
    "    for year in all_years:\n",
    "        missing_data.append({\n",
    "            'country_id': country_id, \n",
    "            'life_ex': yearly_life_mean[year], \n",
    "            'year': year\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "df_avg_life = pd.concat([df_avg_life, missing_df])\n",
    "\n",
    "df_avg_life = df_avg_life[(df_avg_life['country_id'].notna())&(df_avg_life['country_id']!=0)]\n",
    "df_avg_life.to_csv(path_4_risk_assess + 'hi/life_ex.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consume\n",
    "# 海鱼和淡水鱼\n",
    "# Sea fish and freshwater fish\n",
    "df_country = pd.read_excel(path_country_csv + \"hi_raw.xlsx\",sheet_name='country')\n",
    "dic_code = df_country.set_index(\"country_ename\")[\"country_id\"].to_dict()\n",
    "\n",
    "df_consume = pd.read_excel(path_country_csv + \"hi_raw.xlsx\",sheet_name='consume')\n",
    "df_consume = df_consume[(df_consume['Year Code']>1999)&(df_consume['Year Code']<2021)]\n",
    "df_consume = df_consume.rename(columns={'Year Code':'year'})\n",
    "\n",
    "df_consume_sf = df_consume[df_consume['Item']=='Marine Fish, Other']\n",
    "df_consume_ff = df_consume[df_consume['Item']=='Freshwater Fish']\n",
    "\n",
    "df_consume_sf[\"country_id\"] = df_consume_sf[\"country_ename\"].map(dic_code)\n",
    "df_consume_sf_avg = df_consume_sf[['country_id','consume','year']]\n",
    "yearly_sf_mean = df_consume_sf_avg.groupby('year')['consume'].mean()\n",
    "\n",
    "all_country_ids = df_country['country_id'].unique()\n",
    "all_years = df_consume_sf_avg['year'].unique()\n",
    "all_combinations = pd.DataFrame([(country, year) for country in all_country_ids for year in all_years], \n",
    "                                columns=['country_id', 'year'])\n",
    "df_consume_sf_avg = pd.merge(all_combinations, df_consume_sf_avg, on=['country_id', 'year'], how='left')\n",
    "\n",
    "df_consume_sf_avg.to_csv(path_4_risk_assess + 'hi/sf_consume.csv',index=False)\n",
    "\n",
    "df_consume_ff[\"country_id\"] = df_consume_ff[\"country_ename\"].map(dic_code)\n",
    "df_consume_ff_avg = df_consume_ff[['country_id','consume','year']]\n",
    "\n",
    "yearly_ff_mean = df_consume_ff_avg.groupby('year')['consume'].mean()\n",
    "df_consume_ff_avg = pd.merge(all_combinations, df_consume_ff_avg, on=['country_id', 'year'], how='left')\n",
    "df_consume_ff_avg.to_csv(path_4_risk_assess + 'hi/ff_consume.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize grid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_id = pd.read_csv(path_4_risk_assess_grid + 'country_id.csv')\n",
    "df_country_id = df_country_id[df_country_id['country_id']!=0]\n",
    "df_coast = pd.read_csv(path_4_risk_assess_grid + 'coast_distance.csv')\n",
    "\n",
    "df_type = pd.read_csv(path_4_risk_assess_grid + 'type.csv')\n",
    "df_merged = pd.merge(df_country_id, df_type, on=['lon', 'lat'], how='left')\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_coast, on=['lon', 'lat'], how='left')\n",
    "pollution_nan = df_merged['population'].isna().sum()\n",
    "print(f\"Number of unmatched pollution data: {pollution_nan}\")\n",
    "type_nan = df_merged['type'].isna().sum()\n",
    "print(f\"Number of unmatched type data: {type_nan}\")\n",
    "type_nan = df_merged['coast_distance'].isna().sum()\n",
    "print(f\"Number of unmatched type data: {type_nan}\")\n",
    "\n",
    "print(df_merged.columns)\n",
    "df_merged.to_csv(path_4_risk_assess_grid + 'grid.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type = pd.read_csv(path_4_risk_assess_grid + 'type.csv')\n",
    "df_sw = pd.read_csv(path_4_risk_assess + 'pfas/sw_concentration.csv')\n",
    "df_sw = df_sw.rename(columns={'lon_grid':'lon','lat_grid':'lat'})\n",
    "\n",
    "df_grid = pd.read_csv(path_4_risk_assess_grid + 'grid.csv')\n",
    "\n",
    "df_sw_type = pd.merge(df_sw, df_type, on=['lon', 'lat'], how='left')\n",
    "df_sw_type = df_sw_type.rename(columns={'type':'forecast_type'})\n",
    "df_sw_type = df_sw_type[['lon','lat','forecast_type']]\n",
    "df_grid_final = pd.merge(df_grid, df_sw_type, on=['lon', 'lat'], how='left')\n",
    "\n",
    "df_grid_final.to_csv(path_4_risk_assess_grid + 'new_grid.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371\n",
    "    return c * r\n",
    "\n",
    "\n",
    "def get_nearest_grid(df_need, df_search):\n",
    "    nearest_grid_lon = []\n",
    "    nearest_grid_lat = []\n",
    "\n",
    "    for idx, row in df_need.iterrows():\n",
    "        lat, lon, country_id = row['lat'], row['lon'], row['country_id']\n",
    "        df_search['distance'] = haversine(lat, lon, df_search['lat'], df_search['lon'])\n",
    "        grid1_same_country = df_search[df_search['country_id'] == country_id].nsmallest(1, 'distance')\n",
    "        if grid1_same_country.empty:\n",
    "            grid1_diff_country = df_search[df_search['country_id'] != country_id].nsmallest(1, 'distance')\n",
    "            nearest_grid_lon.append(grid1_diff_country.iloc[0]['lon'])\n",
    "            nearest_grid_lat.append(grid1_diff_country.iloc[0]['lat'])\n",
    "            \n",
    "        else:\n",
    "            nearest_grid_lon.append(grid1_same_country.iloc[0]['lon'])\n",
    "            nearest_grid_lat.append(grid1_same_country.iloc[0]['lat'])\n",
    "    return nearest_grid_lon, nearest_grid_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid = pd.read_csv(path_4_risk_assess_grid + 'new_grid.csv')\n",
    "df_grid_need = df_grid[(df_grid['population'].notna()) & (df_grid['population']>0)]\n",
    "print(df_grid.shape)\n",
    "print(df_grid_need.shape)\n",
    "\n",
    "df_need1 = df_grid_need.copy()\n",
    "print(df_need1.shape)\n",
    "df_need1 = df_need1[['lon','lat','country_id']]\n",
    "\n",
    "df_search1 = df_grid[(df_grid['type']==1)&(df_grid['forecast_type'].notna())]\n",
    "print(df_search1.shape)\n",
    "df_search1 = df_search1[['lon','lat','country_id']]\n",
    "\n",
    "df_need2 = df_grid_need.copy()\n",
    "print(df_need2.shape)\n",
    "df_need2 = df_need2[['lon','lat','country_id']]\n",
    "\n",
    "df_search2 = df_grid[(df_grid['type']==0)&(df_grid['forecast_type'].notna())]\n",
    "print(df_search2.shape)\n",
    "df_search2 = df_search2[['lon','lat','country_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_nearest_grid_lon, sf_nearest_grid_lat = get_nearest_grid(df_need1, df_search1)\n",
    "df_need1['sf_nearest_lon'] = sf_nearest_grid_lon\n",
    "df_need1['sf_nearest_lat'] = sf_nearest_grid_lat\n",
    "\n",
    "df_need1 = df_need1[['lon','lat','sf_nearest_lon', 'sf_nearest_lat']]\n",
    "\n",
    "ff_nearest_grid_lon, ff_nearest_grid_lat = get_nearest_grid(df_need2, df_search2)\n",
    "df_need2['ff_nearest_lon'] = ff_nearest_grid_lon\n",
    "df_need2['ff_nearest_lat'] = ff_nearest_grid_lat\n",
    "\n",
    "df_need2 = df_need2[['lon','lat','ff_nearest_lon', 'ff_nearest_lat']]\n",
    "\n",
    "df_neareat = pd.merge(df_need1, df_need2, on=['lon', 'lat'], how='left')\n",
    "df_neareat.to_csv(path_4_risk_assess_grid + 'nearest_grid.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_grid_pop = pd.read_csv(path_4_risk_assess_grid + 'new_grid.csv')\n",
    "df_grid_pop = df_grid_pop[['lon', 'lat', 'country_id', 'type', 'coast_distance', 'forecast_type']]\n",
    "df_pop_year = pd.read_csv(path_4_risk_assess_grid + 'population_year.csv')\n",
    "merged_df = pd.merge(df_pop_year, df_grid_pop, on=['lon', 'lat'], how='left')\n",
    "merged_df.to_csv(path_4_risk_assess_grid + 'new_grid_year.csv',index=False)\n",
    "pop_summary = merged_df.groupby(['country_id', 'year'])['pop'].sum().reset_index()\n",
    "pop_summary.to_csv(path_4_risk_assess + '/hi/country_pop.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_country = pd.read_csv(path_4_risk_assess + '/hi/country_pop.csv')\n",
    "df_country = df_country[df_country['pop'] > 0]\n",
    "\n",
    "df_ff_consume = pd.read_csv(path_4_risk_assess + '/hi/ff_consume.csv')\n",
    "df_ff_consume['consume'] = df_ff_consume['consume'] / 365\n",
    "df_ff_consume = df_ff_consume.rename(columns={'consume': 'ff_consume'})\n",
    "\n",
    "df_sf_consume = pd.read_csv(path_4_risk_assess + '/hi/sf_consume.csv')\n",
    "df_sf_consume['consume'] = df_sf_consume['consume'] / 365\n",
    "df_sf_consume = df_sf_consume.rename(columns={'consume': 'sf_consume'})\n",
    "\n",
    "df_country = pd.merge(df_country, df_ff_consume[['country_id', 'year', 'ff_consume']], \n",
    "                      on=['country_id', 'year'], how='left')\n",
    "df_country = pd.merge(df_country, df_sf_consume[['country_id', 'year', 'sf_consume']], \n",
    "                      on=['country_id', 'year'], how='left')\n",
    "\n",
    "\n",
    "def fill_missing(df, column):\n",
    "    df[column] = df.groupby('country_id')[column].transform(lambda x: x.fillna(x.median()))\n",
    "    global_median = df.groupby('year')[column].transform('median')\n",
    "    df[column] = df[column].fillna(global_median)\n",
    "    return df\n",
    "\n",
    "df_country = fill_missing(df_country, 'ff_consume')\n",
    "df_country = fill_missing(df_country, 'sf_consume')\n",
    "\n",
    "df_country['ff_consume_sum'] = df_country[\"ff_consume\"] * df_country[\"pop\"]\n",
    "df_country['sf_consume_sum'] = df_country[\"sf_consume\"] * df_country[\"pop\"]\n",
    "\n",
    "df_country.to_csv(path_4_risk_assess + '/hi/avg_consume_median.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_year_data(df_grid, df_avg_consume, year):\n",
    "    df_grid_year = df_grid[df_grid['year'] == year]\n",
    "    df_avg_consume_year = df_avg_consume[df_avg_consume['year'] == year]\n",
    "    \n",
    "    df_grid_inland = df_grid_year[(df_grid_year['type']==0) | (df_grid_year['pop'].notna())]\n",
    "\n",
    "    dict_sf = df_avg_consume_year.set_index(\"country_id\")[\"sf_consume\"].to_dict()\n",
    "    dict_ff = df_avg_consume_year.set_index(\"country_id\")[\"ff_consume\"].to_dict()\n",
    "\n",
    "    df_grid_inland['sf_consume'] = df_grid_inland[\"country_id\"].map(dict_sf)\n",
    "    df_grid_inland['ff_consume'] = df_grid_inland[\"country_id\"].map(dict_ff)\n",
    "\n",
    "    df_grid_inland['ad_fa1_num'] = 0.22222 * np.exp(-0.002 * df_grid_inland['coast_distance'])\n",
    "    \n",
    "    max_val = df_grid_inland['ad_fa1_num'].max()\n",
    "    min_val = df_grid_inland['ad_fa1_num'].min()\n",
    "    list_id = df_grid_inland['country_id'].unique().tolist()\n",
    "    \n",
    "    df_grid_inland['ad_fa_per_sf'] = (df_grid_inland['ad_fa1_num'] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    for country_id in list_id:\n",
    "        type_0_count = df_grid_inland[(df_grid_inland['country_id'] == country_id) & (df_grid_inland['type'] == 0)].shape[0]\n",
    "        \n",
    "        if type_0_count == 1:\n",
    "            df_grid_inland.loc[(df_grid_inland['country_id'] == country_id), 'ad_fa_per_ff'] = 1\n",
    "        else:\n",
    "            df_grid_inland.loc[(df_grid_inland['country_id'] == country_id), 'ad_fa_per_ff'] = 1 - df_grid_inland.loc[(df_grid_inland['country_id'] == country_id), 'ad_fa_per_sf']\n",
    "\n",
    "    df_grid_inland['re1_sf_consume'] = df_grid_inland['sf_consume'] * df_grid_inland['ad_fa_per_sf']\n",
    "    df_grid_inland['re1_ff_consume'] = df_grid_inland['ff_consume'] * df_grid_inland['ad_fa_per_ff']\n",
    "\n",
    "    df_grid_inland['re1_sf_consume_sum'] = df_grid_inland['re1_sf_consume'] * df_grid_inland['pop']\n",
    "    df_grid_inland['re1_ff_consume_sum'] = df_grid_inland['re1_ff_consume'] * df_grid_inland['pop']\n",
    "\n",
    "    sf_consume_sum_dict = df_grid_inland.groupby('country_id')['re1_sf_consume_sum'].sum().to_dict()\n",
    "    ff_consume_sum_dict = df_grid_inland.groupby('country_id')['re1_ff_consume_sum'].sum().to_dict()\n",
    "\n",
    "    df_avg_consume_year['re1_sf_consume_sum'] = df_avg_consume_year['country_id'].map(sf_consume_sum_dict)\n",
    "    df_avg_consume_year['re1_ff_consume_sum'] = df_avg_consume_year['country_id'].map(ff_consume_sum_dict)\n",
    "\n",
    "    df_avg_consume_year['tf_ff'] = df_avg_consume_year['ff_consume_sum'] / df_avg_consume_year['re1_ff_consume_sum']\n",
    "    df_avg_consume_year['tf_ff'].fillna(1, inplace=True)\n",
    "\n",
    "    df_avg_consume_year['tf_sf'] = df_avg_consume_year['sf_consume_sum'] / df_avg_consume_year['re1_sf_consume_sum']\n",
    "    df_avg_consume_year['tf_sf'].fillna(1, inplace=True)\n",
    "\n",
    "    dict_sf_fa = df_avg_consume_year.set_index(\"country_id\")[\"tf_sf\"].to_dict()\n",
    "    dict_ff_fa = df_avg_consume_year.set_index(\"country_id\")[\"tf_ff\"].to_dict()\n",
    "\n",
    "    df_grid_inland['tf_sf'] = df_grid_inland['country_id'].map(dict_sf_fa)\n",
    "    df_grid_inland['tf_ff'] = df_grid_inland['country_id'].map(dict_ff_fa)\n",
    "\n",
    "    df_grid_inland['re2_sf_consume'] = df_grid_inland['re1_sf_consume'] * df_grid_inland['tf_sf']\n",
    "    df_grid_inland['re2_ff_consume'] = df_grid_inland['re1_ff_consume'] * df_grid_inland['tf_ff']\n",
    "\n",
    "    df_grid_inland_output = df_grid_inland[['lon','lat','country_id', 'type','re2_sf_consume','re2_ff_consume','pop','coast_distance', 'year']]\n",
    "    df_grid_inland_output = df_grid_inland_output.rename(columns={'re2_sf_consume':'sf_consume','re2_ff_consume':'ff_consume'})\n",
    "    df_grid_inland_output = df_grid_inland_output[df_grid_inland_output['pop']>0]\n",
    "    \n",
    "    return df_grid_inland_output\n",
    "\n",
    "df_grid = pd.read_csv(path_4_risk_assess_grid + 'new_grid_year.csv')\n",
    "df_avg_consume = pd.read_csv(path_4_risk_assess + '/hi/avg_consume_median.csv')\n",
    "\n",
    "years = df_grid['year'].unique()\n",
    "\n",
    "results = []\n",
    "for year in years:\n",
    "    result = process_year_data(df_grid, df_avg_consume, year)\n",
    "    results.append(result)\n",
    "\n",
    "final_result = pd.concat(results, ignore_index=True)\n",
    "\n",
    "final_result.to_csv(path_4_risk_assess_grid + 'consume_grid_year.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid PFAS concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_grid = pd.read_csv(path_4_risk_assess_grid + 'consume_grid_year.csv')\n",
    "\n",
    "df_nearest = pd.read_csv(path_4_risk_assess_grid + 'nearest_grid.csv')\n",
    "\n",
    "df_grid = df_grid.merge(df_nearest, on=['lon', 'lat'], how='left')\n",
    "\n",
    "path_5_sw = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part3_forecast/sw_forecast/s7_rf_output/only_pfas/\"\n",
    "path_5_lr = drive_letter + \"/wyy/code_project/running_outcome/final_data/SPDB/part3_forecast/lr_forecast/s7_rf_output/only_pfas/\"\n",
    "\n",
    "for str_pfas in ['FOSA', 'PFBA',\n",
    "       'PFBS', 'PFDA', 'PFDoDA', 'PFHpA', 'PFHxA', 'PFHxS', 'PFNA', 'PFOA',\n",
    "       'PFOS', 'PFPeA', 'PFTeDA', 'PFTrDA', 'PFUnDA']:\n",
    "   df_lr = pd.read_csv(path_5_lr + 'lr_'+str_pfas+'.csv')\n",
    "   df_sw = pd.read_csv(path_5_sw + 'sw_'+str_pfas+'.csv')\n",
    "\n",
    "   for df_select in [df_lr, df_sw]:\n",
    "      if df_select is df_lr:\n",
    "         str_marke = 'lr'\n",
    "         df_data = df_lr.copy()\n",
    "      else:\n",
    "         str_marke = 'sw'\n",
    "         df_data = df_sw.copy()\n",
    "      print(str_marke)\n",
    "      df_grid_pfas = df_grid.copy()\n",
    "      print(df_grid_pfas.columns)\n",
    "      list_remain = ['lon','lat','year','country_id','ff_consume','sf_consume',str_pfas + '_min',str_pfas + '_max',str_pfas + '_median']\n",
    "      df_grid_pfas['sw_consume'] = 2*0.2*0.5\n",
    "\n",
    "      df_grid_pfas = df_grid_pfas.merge(df_data, left_on=['ff_nearest_lon', 'ff_nearest_lat', 'year'], right_on=['lon_grid', 'lat_grid', 'year'], how='left')\n",
    "      df_grid_pfas = df_grid_pfas[list_remain]\n",
    "      df_grid_pfas.to_csv(path_4_risk_assess + 'pfas/' +str_marke + '_' + str_pfas+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
